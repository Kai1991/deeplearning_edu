{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "import os\n",
    "import math\n",
    "import cPickle\n",
    "import random\n",
    "\n",
    "token_file = 'results_20130124.token'\n",
    "output_dir = 'dir_runs/06091317'\n",
    "input_filenamepatten = 'features/*'\n",
    "vocab_file = 'vocab.txt'\n",
    "hp_config = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_token_file(token_file):\n",
    "    image_name_to_tokens = {}\n",
    "    with open(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        image_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        image_name, _ = image_id.split('#')\n",
    "        image_name_to_tokens.setdefault(image_name, [])\n",
    "        image_name_to_tokens[image_name].append(description)\n",
    "    return image_name_to_tokens\n",
    "  \n",
    "def convert_token_to_id(image_name_to_tokens, vocab):\n",
    "    image_name_to_token_ids = {}\n",
    "    for image_name in image_name_to_tokens:\n",
    "        image_name_to_token_ids.setdefault(image_name, [])\n",
    "        descriptions = image_name_to_tokens[image_name]\n",
    "        for description in descriptions:\n",
    "            token_ids = vocab.encode(description)\n",
    "            image_name_to_token_ids[image_name].append(token_ids)\n",
    "    return image_name_to_token_ids\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with tf.gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            occurence = int(occurence)\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception('duplicate words in vocab file')\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word[cur_id]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionData(object):\n",
    "    def __init__(self,\n",
    "                 image_name_to_token_ids,\n",
    "                 image_feature_filepattern,\n",
    "                 num_timesteps,\n",
    "                 vocab):\n",
    "        self._vocab = vocab\n",
    "        self._all_image_feature_filenames = tf.gfile.Glob(image_feature_filepattern)\n",
    "        self._image_name_to_token_ids = image_name_to_token_ids\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._indicator = 0\n",
    "        self._image_feature_filenames = []\n",
    "        self._image_feature_data = []\n",
    "        self._load_image_feature_pickle()\n",
    "\n",
    "\n",
    "    def _load_image_feature_pickle(self):\n",
    "        for filename in self._all_image_feature_filenames:\n",
    "            tf.logging.info(\"loading %s\" % filename)\n",
    "            with tf.gfile.GFile(filename, 'r') as f:\n",
    "                filenames, features = cPickle.load(f)\n",
    "                self._image_feature_filenames += filenames\n",
    "                self._image_feature_data.append(features)\n",
    "        self._image_feature_data = np.vstack(self._image_feature_data)\n",
    "        self._image_feature_filenames = np.asarray(self._image_feature_filenames)\n",
    "        print self._image_feature_data.shape\n",
    "        print self._image_feature_filenames.shape\n",
    "\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._image_feature_filenames)\n",
    "\n",
    "    def image_feature_size(self):\n",
    "        return (192, 8)\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._image_feature_filenames = self._image_feature_filenames[p]\n",
    "        self._image_feature_data = self._image_feature_data[p]\n",
    "\n",
    "    def _image_desc(self, filenames):\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in filenames:\n",
    "            token_ids_set = self._image_name_to_token_ids[filename]\n",
    "            chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            if chosen_token_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        if self._indicator + batch_size > self.size():\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator <= self.size()\n",
    "\n",
    "        batch_image_features = self._image_feature_data[self._indicator: end_indicator]\n",
    "        batch_image_features = np.reshape(batch_image_features, (batch_size, 192, 8))\n",
    "        batch_sentence_ids, batch_weights = self._image_desc(\n",
    "            self._image_feature_filenames[self._indicator: end_indicator])\n",
    "\n",
    "        self._indicator = end_indicator\n",
    "        return batch_image_features, batch_sentence_ids, batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold=5,\n",
    "        num_embedding_nodes=16,\n",
    "        num_timesteps=10,\n",
    "        num_lstm_nodes=[32, 32],\n",
    "        num_lstm_layers=2,\n",
    "        num_fc_nodes=32,\n",
    "        batch_size=5,\n",
    "        cell_type='lstm',\n",
    "        clip_lstm_grads=1.0,\n",
    "        learning_rate=0.001,\n",
    "        keep_prob=1.0,\n",
    "        log_frequent=100,\n",
    "        save_frequent=2000,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hps, vocab_size, image_feature_shape):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    image_feature  = tf.placeholder(tf.float32,\n",
    "                                    (batch_size, image_feature_shape[0], image_feature_shape[1]))\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.float32, (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    \n",
    "    with tf.variable_scope('initial_lstm'):\n",
    "        # image_feature_mean: [batch_size, image_feature_shape[1]]\n",
    "        image_feature_mean = tf.reduce_mean(image_feature, 1)\n",
    "        \n",
    "        # h: [batch_size, hps.num_lstm_nodes[-1]]\n",
    "        h = tf.layers.dense(image_feature_mean, \n",
    "                            hps.num_lstm_nodes[-1], \n",
    "                            activation=tf.nn.tanh,\n",
    "                            name = 'image_feature_hidden')\n",
    "        image_feature_flatten = tf.reshape(image_feature, [-1, image_feature_shape[1]])\n",
    "        image_feature_proj = tf.layers.dense(image_feature_flatten,\n",
    "                                             image_feature_shape[1])\n",
    "        # image_feature_proj: [batch_size, image_feature_shape[0], image_feature_shape[1]]\n",
    "        image_feature_proj = tf.reshape(image_feature_proj,\n",
    "                                        [-1, image_feature_shape[0], image_feature_shape[1]])\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        embed_token_ids = tf.nn.embedding_lookup(embeddings, sentence[:, 0:num_timesteps-1])\n",
    "  \n",
    "    image_feature_embed_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('image_feature_embed', initializer=image_feature_embed_init):\n",
    "        embed_img = tf.layers.dense(image_feature_mean, hps.num_embedding_nodes)\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis=1)\n",
    "\n",
    "    # Sets up LSTM network.\n",
    "    scale = 1.0 / math.sqrt(\n",
    "      hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        cross_entropys = 0\n",
    "        generated_words = []\n",
    "        state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        for i in range(hps.num_timesteps):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            \n",
    "            # Generate attention.\n",
    "            context = ...\n",
    "            \n",
    "            \n",
    "            # embed_input: [batch_size, 1, hps.num_embedding_nodes]\n",
    "            embed_input = embed_inputs[:, i, :]\n",
    "            embed_input = tf.reshape(embed_input, [batch_size, hps.num_embedding_nodes])\n",
    "            context_embed_input = tf.concat([embed_input, context], 1)\n",
    "            \n",
    "            # rnn_output: [batch_size, hps.num_lstm_node[-1]]\n",
    "            h, state = cell(context_embed_input, state)\n",
    "            fc1 = tf.layers.dense(h, hps.num_fc_nodes, name='fc1')\n",
    "            fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "            fc1_dropout = tf.nn.relu(fc1_dropout)\n",
    "            # logit: [batch_size, class_num]\n",
    "            logit = tf.layers.dense(fc1_dropout, vocab_size, name='logit')\n",
    "            max_prob_word = tf.argmax(logit, axis=1)\n",
    "            max_prob_word = tf.expand_dims(max_prob_word, 1)\n",
    "            generated_words.append(max_prob_word)\n",
    "            word_label = sentence[:, i]\n",
    "            word_mask = mask[:, i]\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=logit, labels=word_label)\n",
    "            cross_entropys += tf.reduce_sum(tf.multiply(cross_entropy, word_mask))\n",
    "\n",
    "        loss = cross_entropys / tf.reduce_sum(mask)\n",
    "        generated_words = tf.concat(generated_words, 1)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info(\"variable name: %s\" % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        for grad, var in zip(grads, tvars):\n",
    "            tf.summary.histogram('%s_grad' % (var.name), grad)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step=global_step)\n",
    "\n",
    "    return ((image_feature, sentence, mask, keep_prob),\n",
    "            (loss, generated_words, train_op),\n",
    "            global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = get_default_params().parse(hp_config)\n",
    "\n",
    "\n",
    "output_dir = output_dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "vocab = Vocab(vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "tf.logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "\n",
    "image_name_to_tokens = parse_token_file(token_file)\n",
    "image_name_to_token_ids = convert_token_to_id(image_name_to_tokens, vocab)\n",
    "\n",
    "data = ImageCaptionData(image_name_to_token_ids,\n",
    "                        input_filenamepatten,\n",
    "                        hps.num_timesteps,\n",
    "                        vocab)\n",
    "image_feature_dim = data.image_feature_size()\n",
    "tf.logging.info(\"image_feature_dim: %d, %d\" % image_feature_dim)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    placeholders, metrics, global_step = create_model(hps, vocab_size, image_feature_dim)\n",
    "    image_feature, sentence, mask, keep_prob = placeholders\n",
    "    loss, generated_words, train_op = metrics\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(max_to_keep=10)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "\n",
    "        tf.logging.info(\"[*] Reading checkpoint ...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(output_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, os.path.join(output_dir, ckpt_name))\n",
    "            tf.logging.info(\"[*] Success Read Checkpoint From %s\" % (ckpt_name))\n",
    "        else:\n",
    "            tf.logging.info(\"[*] Failed load checkpoint\")\n",
    "\n",
    "        last_log_step = -1\n",
    "        last_save_step = -1\n",
    "        for i in range(100000):\n",
    "            batch_image_features, batch_sentence_ids, batch_weights = data.next(hps.batch_size)\n",
    "            input_vals = (batch_image_features, batch_sentence_ids, batch_weights, hps.keep_prob)\n",
    "            feed_dict = dict(zip(placeholders, input_vals))\n",
    "\n",
    "            should_log = last_log_step == -1 or (\n",
    "                global_step_val - last_log_step >= hps.log_frequent)\n",
    "            fetches = [global_step, loss, train_op, summary_op]\n",
    "\n",
    "            if should_log:\n",
    "                fetches += [generated_words]\n",
    "            outputs = sess.run(fetches, feed_dict)\n",
    "            global_step_val, loss_val = outputs[0:2]\n",
    "\n",
    "            if should_log:\n",
    "                summary_str, generated_words_val = outputs[3:]\n",
    "                equal = (generated_words_val == batch_sentence_ids)\n",
    "                weight_equal = equal * batch_weights\n",
    "                accuracy = np.sum(weight_equal) / (np.sum(batch_weights) * 1.0)\n",
    "                writer.add_summary(summary_str, global_step_val)\n",
    "                tf.logging.info(\n",
    "                    'Step: %5d, last_step: %5d, loss: %3.5f, word_accuracy: %3.5f' \n",
    "                    % (global_step_val, last_log_step, loss_val, accuracy))\n",
    "                last_log_step = global_step_val\n",
    "            should_save = last_save_step == -1 or (\n",
    "                global_step_val - last_save_step >= hps.save_frequent)\n",
    "            if should_save:\n",
    "                if last_save_step != -1:\n",
    "                    tf.logging.info(\"Step: %d, text classify model saved\" \n",
    "                                    % (global_step_val))\n",
    "                saver.save(sess, os.path.join(output_dir, \"lstm\"), \n",
    "                           global_step=global_step_val)\n",
    "                last_save_step = global_step_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
