{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow af tf\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import cPickle\n",
    "import time\n",
    "\n",
    "slim = tf.contrib.slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vocab(input_token_filename):\n",
    "  with open(input_token_filename, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "  max_length_of_sentences = 0\n",
    "  length_dict = {}\n",
    "\n",
    "  vocab_dict = {}\n",
    "  for line in lines:\n",
    "    image_id, description = line.strip('\\n').split('\\t')\n",
    "    words = description.strip(' ').split()\n",
    "    max_length_of_sentences = max(max_length_of_sentences, len(words))\n",
    "    length_dict.setdefault(len(words), 0)\n",
    "    length_dict[len(words)] += 1\n",
    "\n",
    "    for word in words:\n",
    "      vocab_dict.setdefault(word, 0)\n",
    "      vocab_dict[word] += 1\n",
    "  print max_length_of_sentences\n",
    "  pprint.pprint(length_dict)\n",
    "  return vocab_dict\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  if len(sys.argv) != 3:\n",
    "    print 'python %s <input_token_filename> <output_vocabulary_filename>'\n",
    "    sys.exit(-1)\n",
    "\n",
    "  input_token_filename = sys.argv[1]\n",
    "  output_vocabulary_filename = sys.argv[2]\n",
    "\n",
    "  vocab_dict = count_vocab(input_token_filename)\n",
    "  sorted_vocab_dict = sorted(vocab_dict.items(), key = lambda d:d[1], reverse=True)\n",
    "  with open(output_vocabulary_filename, 'w') as f:\n",
    "    f.write(\"<UNK>\\t100000\\n\")\n",
    "    for item in sorted_vocab_dict:\n",
    "      f.write('%s\\t%d\\n' % item)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 2
}
