{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "import os\n",
    "import math\n",
    "import cPickle\n",
    "import random\n",
    "\n",
    "token_file = 'results_20130124.token'\n",
    "output_dir = 'dir_runs/06091317'\n",
    "input_filenamepatten = 'features/*'\n",
    "vocab_file = 'vocab.txt'\n",
    "hp_config = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_token_file(token_file):\n",
    "    image_name_to_tokens = {}\n",
    "    with open(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        image_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        image_name, _ = image_id.split('#')\n",
    "        image_name_to_tokens.setdefault(image_name, [])\n",
    "        image_name_to_tokens[image_name].append(description)\n",
    "    return image_name_to_tokens\n",
    "  \n",
    "def convert_token_to_id(image_name_to_tokens, vocab):\n",
    "    image_name_to_token_ids = {}\n",
    "    for image_name in image_name_to_tokens:\n",
    "        image_name_to_token_ids.setdefault(image_name, [])\n",
    "        descriptions = image_name_to_tokens[image_name]\n",
    "        for description in descriptions:\n",
    "            token_ids = vocab.encode(description)\n",
    "            image_name_to_token_ids[image_name].append(token_ids)\n",
    "    return image_name_to_token_ids\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with tf.gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            occurence = int(occurence)\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception('duplicate words in vocab file')\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word[cur_id]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionData(object):\n",
    "    def __init__(self,\n",
    "                 image_name_to_token_ids,\n",
    "                 image_feature_filepattern,\n",
    "                 num_timesteps,\n",
    "                 vocab):\n",
    "        self._vocab = vocab\n",
    "        self._all_image_feature_filenames = tf.gfile.Glob(image_feature_filepattern)\n",
    "        self._image_name_to_token_ids = image_name_to_token_ids\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._indicator = 0\n",
    "        self._image_feature_filenames = []\n",
    "        self._image_feature_data = []\n",
    "        self._load_image_feature_pickle()\n",
    "\n",
    "\n",
    "    def _load_image_feature_pickle(self):\n",
    "        for filename in self._all_image_feature_filenames:\n",
    "            tf.logging.info(\"loading %s\" % filename)\n",
    "            with tf.gfile.GFile(filename, 'r') as f:\n",
    "                filenames, features = cPickle.load(f)\n",
    "                self._image_feature_filenames += filenames\n",
    "                self._image_feature_data.append(features)\n",
    "        self._image_feature_data = np.vstack(self._image_feature_data)\n",
    "        self._image_feature_filenames = np.asarray(self._image_feature_filenames)\n",
    "        print self._image_feature_data.shape\n",
    "        print self._image_feature_filenames.shape\n",
    "\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._image_feature_filenames)\n",
    "\n",
    "    def image_feature_size(self):\n",
    "        return (192, 8)\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._image_feature_filenames = self._image_feature_filenames[p]\n",
    "        self._image_feature_data = self._image_feature_data[p]\n",
    "\n",
    "    def _image_desc(self, filenames):\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in filenames:\n",
    "            token_ids_set = self._image_name_to_token_ids[filename]\n",
    "            chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            if chosen_token_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        if self._indicator + batch_size > self.size():\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator <= self.size()\n",
    "\n",
    "        batch_image_features = self._image_feature_data[self._indicator: end_indicator]\n",
    "        batch_image_features = np.reshape(batch_image_features, (batch_size, 192, 8))\n",
    "        batch_sentence_ids, batch_weights = self._image_desc(\n",
    "            self._image_feature_filenames[self._indicator: end_indicator])\n",
    "\n",
    "        self._indicator = end_indicator\n",
    "        return batch_image_features, batch_sentence_ids, batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold=5,\n",
    "        num_embedding_nodes=16,\n",
    "        num_timesteps=10,\n",
    "        num_lstm_nodes=[32, 32],\n",
    "        num_lstm_layers=2,\n",
    "        num_fc_nodes=32,\n",
    "        batch_size=5,\n",
    "        cell_type='lstm',\n",
    "        clip_lstm_grads=1.0,\n",
    "        learning_rate=0.001,\n",
    "        keep_prob=1.0,\n",
    "        log_frequent=100,\n",
    "        save_frequent=2000,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hps, vocab_size, image_feature_shape):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    image_feature  = tf.placeholder(tf.float32,\n",
    "                                    (batch_size, image_feature_shape[0], image_feature_shape[1]))\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.float32, (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "    \n",
    "    with tf.variable_scope('initial_lstm'):\n",
    "        # image_feature_mean: [batch_size, image_feature_shape[1]]\n",
    "        image_feature_mean = tf.reduce_mean(image_feature, 1)\n",
    "        \n",
    "        # h: [batch_size, hps.num_lstm_nodes[-1]]\n",
    "        h = tf.layers.dense(image_feature_mean, \n",
    "                            hps.num_lstm_nodes[-1], \n",
    "                            activation=tf.nn.tanh,\n",
    "                            name = 'image_feature_hidden')\n",
    "        image_feature_flatten = tf.reshape(image_feature, [-1, image_feature_shape[1]])\n",
    "        image_feature_proj = tf.layers.dense(image_feature_flatten,\n",
    "                                             image_feature_shape[1])\n",
    "        # image_feature_proj: [batch_size, image_feature_shape[0], image_feature_shape[1]]\n",
    "        image_feature_proj = tf.reshape(image_feature_proj,\n",
    "                                        [-1, image_feature_shape[0], image_feature_shape[1]])\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        embed_token_ids = tf.nn.embedding_lookup(embeddings, sentence[:, 0:num_timesteps-1])\n",
    "  \n",
    "    image_feature_embed_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('image_feature_embed', initializer=image_feature_embed_init):\n",
    "        embed_img = tf.layers.dense(image_feature_mean, hps.num_embedding_nodes)\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis=1)\n",
    "\n",
    "    # Sets up LSTM network.\n",
    "    scale = 1.0 / math.sqrt(\n",
    "      hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        cross_entropys = 0\n",
    "        generated_words = []\n",
    "        state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        for i in range(hps.num_timesteps):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            \n",
    "            # attention_input_h_part: [batch_size, image_feautre_shape[1]]\n",
    "            attention_input_h_part = tf.layers.dense(h,\n",
    "                                                     image_feature_shape[1],\n",
    "                                                     name=\"attention_h_part\")\n",
    "            # attention_input: [batch_size, image_feature_shape[0], image_feature_shape[1]]\n",
    "            attention_input = image_feature_proj + tf.expand_dims(attention_input_h_part, 1)\n",
    "            attention_hidden = tf.nn.relu(attention_input)\n",
    "            # attention_output: [batch_size, image_feature_shape[0], 1]\n",
    "            attention_out = tf.layers.dense(attention_input, 1, name=\"attention_out\")\n",
    "            # attention_weights: [batch_size, image_feature_shape[0], 1]\n",
    "            attention_weights = tf.nn.softmax(attention_out)\n",
    "            # context: [batch_size, image_feature_shape[1]]\n",
    "            context = tf.reduce_sum(image_feature * attention_weights, 1)\n",
    "            \n",
    "            \n",
    "            # embed_input: [batch_size, 1, hps.num_embedding_nodes]\n",
    "            embed_input = embed_inputs[:, i, :]\n",
    "            embed_input = tf.reshape(embed_input, [batch_size, hps.num_embedding_nodes])\n",
    "            context_embed_input = tf.concat([embed_input, context], 1)\n",
    "            \n",
    "            # rnn_output: [batch_size, hps.num_lstm_node[-1]]\n",
    "            h, state = cell(context_embed_input, state)\n",
    "            fc1 = tf.layers.dense(h, hps.num_fc_nodes, name='fc1')\n",
    "            fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "            fc1_dropout = tf.nn.relu(fc1_dropout)\n",
    "            # logit: [batch_size, class_num]\n",
    "            logit = tf.layers.dense(fc1_dropout, vocab_size, name='logit')\n",
    "            max_prob_word = tf.argmax(logit, axis=1)\n",
    "            max_prob_word = tf.expand_dims(max_prob_word, 1)\n",
    "            generated_words.append(max_prob_word)\n",
    "            word_label = sentence[:, i]\n",
    "            word_mask = mask[:, i]\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=logit, labels=word_label)\n",
    "            cross_entropys += tf.reduce_sum(tf.multiply(cross_entropy, word_mask))\n",
    "\n",
    "        loss = cross_entropys / tf.reduce_sum(mask)\n",
    "        generated_words = tf.concat(generated_words, 1)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info(\"variable name: %s\" % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        for grad, var in zip(grads, tvars):\n",
    "            tf.summary.histogram('%s_grad' % (var.name), grad)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step=global_step)\n",
    "\n",
    "    return ((image_feature, sentence, mask, keep_prob),\n",
    "            (loss, generated_words, train_op),\n",
    "            global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 8186\n",
      "INFO:tensorflow:loading features/image_features-0.pickle\n",
      "INFO:tensorflow:loading features/image_features-1.pickle\n",
      "(200, 1536)\n",
      "(200,)\n",
      "INFO:tensorflow:image_feature_dim: 192, 8\n",
      "WARNING:tensorflow:From <ipython-input-6-d38a98c8519f>:41: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "INFO:tensorflow:variable name: initial_lstm/image_feature_hidden/kernel:0\n",
      "INFO:tensorflow:variable name: initial_lstm/image_feature_hidden/bias:0\n",
      "INFO:tensorflow:variable name: initial_lstm/dense/kernel:0\n",
      "INFO:tensorflow:variable name: initial_lstm/dense/bias:0\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: fc/attention_h_part/kernel:0\n",
      "INFO:tensorflow:variable name: fc/attention_h_part/bias:0\n",
      "INFO:tensorflow:variable name: fc/attention_out/kernel:0\n",
      "INFO:tensorflow:variable name: fc/attention_out/bias:0\n",
      "INFO:tensorflow:variable name: fc/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: fc/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: fc/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/logit/kernel:0\n",
      "INFO:tensorflow:variable name: fc/logit/bias:0\n",
      "INFO:tensorflow:Summary name initial_lstm/image_feature_hidden/kernel:0_grad is illegal; using initial_lstm/image_feature_hidden/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name initial_lstm/image_feature_hidden/bias:0_grad is illegal; using initial_lstm/image_feature_hidden/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name initial_lstm/dense/kernel:0_grad is illegal; using initial_lstm/dense/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name initial_lstm/dense/bias:0_grad is illegal; using initial_lstm/dense/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name embedding/embeddings:0_grad is illegal; using embedding/embeddings_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/kernel:0_grad is illegal; using image_feature_embed/dense/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/bias:0_grad is illegal; using image_feature_embed/dense/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/attention_h_part/kernel:0_grad is illegal; using fc/attention_h_part/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/attention_h_part/bias:0_grad is illegal; using fc/attention_h_part/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/attention_out/kernel:0_grad is illegal; using fc/attention_out/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/attention_out/bias:0_grad is illegal; using fc/attention_out/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0_grad is illegal; using fc/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0_grad is illegal; using fc/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0_grad is illegal; using fc/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0_grad is illegal; using fc/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/kernel:0_grad is illegal; using fc/fc1/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/bias:0_grad is illegal; using fc/fc1/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logit/kernel:0_grad is illegal; using fc/logit/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logit/bias:0_grad is illegal; using fc/logit/bias_0_grad instead.\n",
      "INFO:tensorflow:[*] Reading checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from dir_runs/06091317/lstm-8001\n",
      "INFO:tensorflow:[*] Success Read Checkpoint From lstm-8001\n",
      "INFO:tensorflow:Step:  8002, last_step:    -1, loss: 10.22828, word_accuracy: 0.10000\n",
      "INFO:tensorflow:Step:  8103, last_step:  8002, loss: 5.34944, word_accuracy: 0.24000\n",
      "INFO:tensorflow:Step:  8204, last_step:  8103, loss: 4.24440, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step:  8305, last_step:  8204, loss: 4.60949, word_accuracy: 0.26000\n",
      "INFO:tensorflow:Step:  8406, last_step:  8305, loss: 3.48954, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step:  8507, last_step:  8406, loss: 4.12919, word_accuracy: 0.20000\n",
      "INFO:tensorflow:Step:  8608, last_step:  8507, loss: 3.95157, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step:  8709, last_step:  8608, loss: 3.94952, word_accuracy: 0.22000\n",
      "INFO:tensorflow:Step:  8810, last_step:  8709, loss: 3.27784, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step:  8911, last_step:  8810, loss: 3.83273, word_accuracy: 0.22000\n",
      "INFO:tensorflow:Step:  9012, last_step:  8911, loss: 4.35927, word_accuracy: 0.20000\n",
      "INFO:tensorflow:Step:  9113, last_step:  9012, loss: 3.26716, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step:  9214, last_step:  9113, loss: 4.07425, word_accuracy: 0.20000\n",
      "INFO:tensorflow:Step:  9315, last_step:  9214, loss: 4.17204, word_accuracy: 0.22000\n",
      "INFO:tensorflow:Step:  9416, last_step:  9315, loss: 3.85342, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step:  9517, last_step:  9416, loss: 4.13692, word_accuracy: 0.20000\n",
      "INFO:tensorflow:Step:  9618, last_step:  9517, loss: 3.45863, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step:  9719, last_step:  9618, loss: 4.66494, word_accuracy: 0.22000\n",
      "INFO:tensorflow:Step:  9820, last_step:  9719, loss: 4.03913, word_accuracy: 0.22000\n",
      "INFO:tensorflow:Step:  9921, last_step:  9820, loss: 3.81313, word_accuracy: 0.20000\n",
      "INFO:tensorflow:Step: 10002, text classify model saved\n",
      "INFO:tensorflow:Step: 10022, last_step:  9921, loss: 3.42900, word_accuracy: 0.24000\n",
      "INFO:tensorflow:Step: 10123, last_step: 10022, loss: 3.50533, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step: 10224, last_step: 10123, loss: 3.06513, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 10325, last_step: 10224, loss: 4.14432, word_accuracy: 0.22000\n",
      "INFO:tensorflow:Step: 10426, last_step: 10325, loss: 2.98502, word_accuracy: 0.40000\n",
      "INFO:tensorflow:Step: 10527, last_step: 10426, loss: 3.21564, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 10628, last_step: 10527, loss: 3.53803, word_accuracy: 0.22000\n",
      "INFO:tensorflow:Step: 10729, last_step: 10628, loss: 3.35033, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 10830, last_step: 10729, loss: 3.32062, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 10931, last_step: 10830, loss: 3.67785, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 11032, last_step: 10931, loss: 2.87488, word_accuracy: 0.44000\n",
      "INFO:tensorflow:Step: 11133, last_step: 11032, loss: 3.94933, word_accuracy: 0.28000\n",
      "INFO:tensorflow:Step: 11234, last_step: 11133, loss: 3.44791, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 11335, last_step: 11234, loss: 3.29564, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 11436, last_step: 11335, loss: 3.38259, word_accuracy: 0.22000\n",
      "INFO:tensorflow:Step: 11537, last_step: 11436, loss: 3.57782, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step: 11638, last_step: 11537, loss: 3.29977, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 11739, last_step: 11638, loss: 3.54044, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step: 11840, last_step: 11739, loss: 3.02180, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 11941, last_step: 11840, loss: 3.25615, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 12002, text classify model saved\n",
      "INFO:tensorflow:Step: 12042, last_step: 11941, loss: 3.66139, word_accuracy: 0.24000\n",
      "INFO:tensorflow:Step: 12143, last_step: 12042, loss: 3.47435, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 12244, last_step: 12143, loss: 3.78950, word_accuracy: 0.26000\n",
      "INFO:tensorflow:Step: 12345, last_step: 12244, loss: 2.63355, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step: 12446, last_step: 12345, loss: 3.02726, word_accuracy: 0.32000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step: 12547, last_step: 12446, loss: 2.94717, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 12648, last_step: 12547, loss: 3.02296, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 12749, last_step: 12648, loss: 3.02257, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step: 12850, last_step: 12749, loss: 3.12611, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step: 12951, last_step: 12850, loss: 3.92588, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 13052, last_step: 12951, loss: 3.75697, word_accuracy: 0.26000\n",
      "INFO:tensorflow:Step: 13153, last_step: 13052, loss: 3.15408, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 13254, last_step: 13153, loss: 2.46893, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step: 13355, last_step: 13254, loss: 2.89413, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step: 13456, last_step: 13355, loss: 2.94386, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 13557, last_step: 13456, loss: 3.14625, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 13658, last_step: 13557, loss: 2.88864, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 13759, last_step: 13658, loss: 2.44883, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step: 13860, last_step: 13759, loss: 2.74763, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step: 13961, last_step: 13860, loss: 2.95545, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 14002, text classify model saved\n",
      "INFO:tensorflow:Step: 14062, last_step: 13961, loss: 2.88790, word_accuracy: 0.26000\n",
      "INFO:tensorflow:Step: 14163, last_step: 14062, loss: 3.62905, word_accuracy: 0.24000\n",
      "INFO:tensorflow:Step: 14264, last_step: 14163, loss: 3.02741, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 14365, last_step: 14264, loss: 2.87600, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 14466, last_step: 14365, loss: 3.45173, word_accuracy: 0.26000\n",
      "INFO:tensorflow:Step: 14567, last_step: 14466, loss: 3.09309, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 14668, last_step: 14567, loss: 2.52780, word_accuracy: 0.44000\n",
      "INFO:tensorflow:Step: 14769, last_step: 14668, loss: 2.55655, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 14870, last_step: 14769, loss: 3.03625, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 14971, last_step: 14870, loss: 3.28815, word_accuracy: 0.20000\n",
      "INFO:tensorflow:Step: 15072, last_step: 14971, loss: 2.69524, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 15173, last_step: 15072, loss: 3.70497, word_accuracy: 0.16000\n",
      "INFO:tensorflow:Step: 15274, last_step: 15173, loss: 3.10213, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 15375, last_step: 15274, loss: 2.60799, word_accuracy: 0.44000\n",
      "INFO:tensorflow:Step: 15476, last_step: 15375, loss: 3.06367, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step: 15577, last_step: 15476, loss: 2.84477, word_accuracy: 0.28000\n",
      "INFO:tensorflow:Step: 15678, last_step: 15577, loss: 2.85876, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 15779, last_step: 15678, loss: 2.34974, word_accuracy: 0.48000\n",
      "INFO:tensorflow:Step: 15880, last_step: 15779, loss: 3.13941, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 15981, last_step: 15880, loss: 3.01782, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 16002, text classify model saved\n",
      "INFO:tensorflow:Step: 16082, last_step: 15981, loss: 2.45258, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step: 16183, last_step: 16082, loss: 3.50462, word_accuracy: 0.26000\n",
      "INFO:tensorflow:Step: 16284, last_step: 16183, loss: 2.58480, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step: 16385, last_step: 16284, loss: 3.29486, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 16486, last_step: 16385, loss: 3.58959, word_accuracy: 0.26000\n",
      "INFO:tensorflow:Step: 16587, last_step: 16486, loss: 3.06864, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 16688, last_step: 16587, loss: 2.84651, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 16789, last_step: 16688, loss: 2.74677, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 16890, last_step: 16789, loss: 2.27614, word_accuracy: 0.52000\n",
      "INFO:tensorflow:Step: 16991, last_step: 16890, loss: 2.97807, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step: 17092, last_step: 16991, loss: 2.77980, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step: 17193, last_step: 17092, loss: 2.66396, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step: 17294, last_step: 17193, loss: 3.03116, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step: 17395, last_step: 17294, loss: 2.54811, word_accuracy: 0.40000\n",
      "INFO:tensorflow:Step: 17496, last_step: 17395, loss: 3.07904, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step: 17597, last_step: 17496, loss: 2.87034, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step: 17698, last_step: 17597, loss: 2.79488, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 17799, last_step: 17698, loss: 2.54683, word_accuracy: 0.48000\n",
      "INFO:tensorflow:Step: 17900, last_step: 17799, loss: 2.41673, word_accuracy: 0.44000\n",
      "INFO:tensorflow:Step: 18001, last_step: 17900, loss: 2.06824, word_accuracy: 0.52000\n",
      "INFO:tensorflow:Step: 18002, text classify model saved\n",
      "INFO:tensorflow:Step: 18102, last_step: 18001, loss: 2.31483, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step: 18203, last_step: 18102, loss: 3.09276, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step: 18304, last_step: 18203, loss: 2.63552, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 18405, last_step: 18304, loss: 2.49732, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step: 18506, last_step: 18405, loss: 2.28461, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step: 18607, last_step: 18506, loss: 2.15456, word_accuracy: 0.48000\n",
      "INFO:tensorflow:Step: 18708, last_step: 18607, loss: 3.22548, word_accuracy: 0.16000\n",
      "INFO:tensorflow:Step: 18809, last_step: 18708, loss: 2.57629, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step: 18910, last_step: 18809, loss: 2.78714, word_accuracy: 0.38000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7ecc0d076936>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mfetches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerated_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mglobal_step_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hps = get_default_params().parse(hp_config)\n",
    "\n",
    "\n",
    "output_dir = output_dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "vocab = Vocab(vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "tf.logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "\n",
    "image_name_to_tokens = parse_token_file(token_file)\n",
    "image_name_to_token_ids = convert_token_to_id(image_name_to_tokens, vocab)\n",
    "\n",
    "data = ImageCaptionData(image_name_to_token_ids,\n",
    "                        input_filenamepatten,\n",
    "                        hps.num_timesteps,\n",
    "                        vocab)\n",
    "image_feature_dim = data.image_feature_size()\n",
    "tf.logging.info(\"image_feature_dim: %d, %d\" % image_feature_dim)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    placeholders, metrics, global_step = create_model(hps, vocab_size, image_feature_dim)\n",
    "    image_feature, sentence, mask, keep_prob = placeholders\n",
    "    loss, generated_words, train_op = metrics\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(max_to_keep=10)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "\n",
    "        tf.logging.info(\"[*] Reading checkpoint ...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(output_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, os.path.join(output_dir, ckpt_name))\n",
    "            tf.logging.info(\"[*] Success Read Checkpoint From %s\" % (ckpt_name))\n",
    "        else:\n",
    "            tf.logging.info(\"[*] Failed load checkpoint\")\n",
    "\n",
    "        last_log_step = -1\n",
    "        last_save_step = -1\n",
    "        for i in range(100000):\n",
    "            batch_image_features, batch_sentence_ids, batch_weights = data.next(hps.batch_size)\n",
    "            input_vals = (batch_image_features, batch_sentence_ids, batch_weights, hps.keep_prob)\n",
    "            feed_dict = dict(zip(placeholders, input_vals))\n",
    "\n",
    "            should_log = last_log_step == -1 or (\n",
    "                global_step_val - last_log_step >= hps.log_frequent)\n",
    "            fetches = [global_step, loss, train_op, summary_op]\n",
    "\n",
    "            if should_log:\n",
    "                fetches += [generated_words]\n",
    "            outputs = sess.run(fetches, feed_dict)\n",
    "            global_step_val, loss_val = outputs[0:2]\n",
    "\n",
    "            if should_log:\n",
    "                summary_str, generated_words_val = outputs[3:]\n",
    "                equal = (generated_words_val == batch_sentence_ids)\n",
    "                weight_equal = equal * batch_weights\n",
    "                accuracy = np.sum(weight_equal) / (np.sum(batch_weights) * 1.0)\n",
    "                writer.add_summary(summary_str, global_step_val)\n",
    "                tf.logging.info(\n",
    "                    'Step: %5d, last_step: %5d, loss: %3.5f, word_accuracy: %3.5f' \n",
    "                    % (global_step_val, last_log_step, loss_val, accuracy))\n",
    "                last_log_step = global_step_val\n",
    "            should_save = last_save_step == -1 or (\n",
    "                global_step_val - last_save_step >= hps.save_frequent)\n",
    "            if should_save:\n",
    "                if last_save_step != -1:\n",
    "                    tf.logging.info(\"Step: %d, text classify model saved\" \n",
    "                                    % (global_step_val))\n",
    "                saver.save(sess, os.path.join(output_dir, \"lstm\"), \n",
    "                           global_step=global_step_val)\n",
    "                last_save_step = global_step_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
