{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import pprint\n",
    "import os\n",
    "import math\n",
    "import cPickle\n",
    "import random\n",
    "\n",
    "token_file = 'results_20130124.token'\n",
    "output_dir = 'dir_runs/06091043'\n",
    "input_filenamepatten = 'features/*'\n",
    "vocab_file = 'vocab.txt'\n",
    "hp_config = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_token_file(token_file):\n",
    "    image_name_to_tokens = {}\n",
    "    with open(token_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        image_id, description = line.strip('\\r\\n').split('\\t')\n",
    "        image_name, _ = image_id.split('#')\n",
    "        image_name_to_tokens.setdefault(image_name, [])\n",
    "        image_name_to_tokens[image_name].append(description)\n",
    "    return image_name_to_tokens\n",
    "  \n",
    "def convert_token_to_id(image_name_to_tokens, vocab):\n",
    "    image_name_to_token_ids = {}\n",
    "    for image_name in image_name_to_tokens:\n",
    "        image_name_to_token_ids.setdefault(image_name, [])\n",
    "        descriptions = image_name_to_tokens[image_name]\n",
    "        for description in descriptions:\n",
    "            token_ids = vocab.encode(description)\n",
    "            image_name_to_token_ids[image_name].append(token_ids)\n",
    "    return image_name_to_token_ids\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, filename, word_num_threshold):\n",
    "        self._id_to_word = {}\n",
    "        self._word_to_id = {}\n",
    "        self._unk = -1\n",
    "        self._eos = -1\n",
    "        self._word_num_threshold = word_num_threshold\n",
    "        self._read_dict(filename)\n",
    "\n",
    "    def _read_dict(self, filename):\n",
    "        with tf.gfile.GFile(filename, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        for line in lines:\n",
    "            word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "            occurence = int(occurence)\n",
    "            if word != '<UNK>' and occurence < self._word_num_threshold:\n",
    "                continue\n",
    "            idx = len(self._id_to_word)\n",
    "            if word == '<UNK>':\n",
    "                self._unk = idx\n",
    "            elif word == '.':\n",
    "                self._eos = idx\n",
    "            if idx in self._id_to_word or word in self._word_to_id:\n",
    "                raise Exception('duplicate words in vocab file')\n",
    "            self._word_to_id[word] = idx\n",
    "            self._id_to_word[idx] = word\n",
    "\n",
    "    @property\n",
    "    def unk(self):\n",
    "        return self._unk\n",
    "\n",
    "    @property\n",
    "    def eos(self):\n",
    "        return self._eos\n",
    "\n",
    "    def word_to_id(self, word):\n",
    "        return self._word_to_id.get(word, self.unk)\n",
    "\n",
    "    def id_to_word(self, cur_id):\n",
    "        return self._id_to_word[cur_id]\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._word_to_id)\n",
    "\n",
    "    def encode(self, sentence):\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "        return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionData(object):\n",
    "    def __init__(self,\n",
    "                 image_name_to_token_ids,\n",
    "                 image_feature_filepattern,\n",
    "                 num_timesteps,\n",
    "                 vocab):\n",
    "        self._vocab = vocab\n",
    "        self._all_image_feature_filenames = tf.gfile.Glob(image_feature_filepattern)\n",
    "        self._image_name_to_token_ids = image_name_to_token_ids\n",
    "        self._num_timesteps = num_timesteps\n",
    "        self._indicator = 0\n",
    "        self._image_feature_filenames = []\n",
    "        self._image_feature_data = []\n",
    "        self._load_image_feature_pickle()\n",
    "\n",
    "\n",
    "    def _load_image_feature_pickle(self):\n",
    "        for filename in self._all_image_feature_filenames:\n",
    "            tf.logging.info(\"loading %s\" % filename)\n",
    "            with tf.gfile.GFile(filename, 'r') as f:\n",
    "                filenames, features = cPickle.load(f)\n",
    "                self._image_feature_filenames += filenames\n",
    "                self._image_feature_data.append(features)\n",
    "        self._image_feature_data = np.vstack(self._image_feature_data)\n",
    "        self._image_feature_filenames = np.asarray(self._image_feature_filenames)\n",
    "        print self._image_feature_data.shape\n",
    "        print self._image_feature_filenames.shape\n",
    "\n",
    "\n",
    "    def size(self):\n",
    "        return len(self._image_feature_filenames)\n",
    "\n",
    "    def image_feature_size(self):\n",
    "        return self._image_feature_data.shape[1]\n",
    "\n",
    "    def _random_shuffle(self):\n",
    "        p = np.random.permutation(self.size())\n",
    "        self._image_feature_filenames = self._image_feature_filenames[p]\n",
    "        self._image_feature_data = self._image_feature_data[p]\n",
    "\n",
    "    def _image_desc(self, filenames):\n",
    "        batch_sentence_ids = []\n",
    "        batch_weights = []\n",
    "        for filename in filenames:\n",
    "            token_ids_set = self._image_name_to_token_ids[filename]\n",
    "            chosen_token_ids = random.choice(token_ids_set)\n",
    "            chosen_token_length = len(chosen_token_ids)\n",
    "\n",
    "            weight = [1 for i in range(chosen_token_length)]\n",
    "            if chosen_token_length >= self._num_timesteps:\n",
    "                chosen_token_ids = chosen_token_ids[0:self._num_timesteps]\n",
    "                weight = weight[0:self._num_timesteps]\n",
    "            else:\n",
    "                remaining_length = self._num_timesteps - chosen_token_length\n",
    "                chosen_token_ids += [self._vocab.eos for i in range(remaining_length)]\n",
    "                weight += [0 for i in range(remaining_length)]\n",
    "            batch_sentence_ids.append(chosen_token_ids)\n",
    "            batch_weights.append(weight)\n",
    "        batch_sentence_ids = np.asarray(batch_sentence_ids)\n",
    "        batch_weights = np.asarray(batch_weights)\n",
    "        return batch_sentence_ids, batch_weights\n",
    "\n",
    "    def next(self, batch_size):\n",
    "        if self._indicator + batch_size > self.size():\n",
    "            self._random_shuffle()\n",
    "            self._indicator = 0\n",
    "\n",
    "        end_indicator = self._indicator + batch_size\n",
    "        assert end_indicator <= self.size()\n",
    "\n",
    "        batch_image_features = self._image_feature_data[self._indicator: end_indicator]\n",
    "        batch_sentence_ids, batch_weights = self._image_desc(\n",
    "            self._image_feature_filenames[self._indicator: end_indicator])\n",
    "\n",
    "        self._indicator = end_indicator\n",
    "        return batch_image_features, batch_sentence_ids, batch_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "    return tf.contrib.training.HParams(\n",
    "        num_vocab_word_threshold=5,\n",
    "        num_embedding_nodes=16,\n",
    "        num_timesteps=10,\n",
    "        num_lstm_nodes=[32, 32],\n",
    "        num_lstm_layers=2,\n",
    "        num_fc_nodes=32,\n",
    "        batch_size=5,\n",
    "        cell_type='lstm',\n",
    "        clip_lstm_grads=1.0,\n",
    "        learning_rate=0.001,\n",
    "        keep_prob=1.0,\n",
    "        log_frequent=100,\n",
    "        save_frequent=2000,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn_cell(hidden_dim, cell_type):\n",
    "    if cell_type == 'lstm':\n",
    "        return tf.contrib.rnn.BasicLSTMCell(hidden_dim, state_is_tuple=True)\n",
    "    elif cell_type == 'gru':\n",
    "        return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "    else:\n",
    "        raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hps, vocab_size, image_feature_dim):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    image_feature  = tf.placeholder(tf.float32, (batch_size, image_feature_dim))\n",
    "    sentence = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    mask = tf.placeholder(tf.float32, (batch_size, num_timesteps))\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "\n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        embed_token_ids = tf.nn.embedding_lookup(embeddings, sentence[:, 0:num_timesteps-1])\n",
    "  \n",
    "    image_feature_embed_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('image_feature_embed', initializer=image_feature_embed_init):\n",
    "        embed_img = tf.layers.dense(image_feature, hps.num_embedding_nodes)\n",
    "        embed_img = tf.expand_dims(embed_img, 1)\n",
    "        embed_inputs = tf.concat([embed_img, embed_token_ids], axis=1)\n",
    "\n",
    "    # Sets up LSTM network.\n",
    "    scale = 1.0 / math.sqrt(\n",
    "      hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        cross_entropys = 0\n",
    "        generated_words = []\n",
    "        for i in range(hps.num_timesteps):\n",
    "            if i > 0:\n",
    "                tf.get_variable_scope().reuse_variables()\n",
    "            embed_input = embed_inputs[:, i, :]\n",
    "            embed_input = tf.reshape(embed_input, [batch_size, hps.num_embedding_nodes])\n",
    "            # rnn_output: [batch_size, hps.num_lstm_node[-1]]\n",
    "            rnn_output, state = cell(embed_input, state)\n",
    "            fc1 = tf.layers.dense(rnn_output, hps.num_fc_nodes, name='fc1')\n",
    "            fc1_dropout = tf.contrib.layers.dropout(fc1, keep_prob)\n",
    "            fc1_dropout = tf.nn.relu(fc1_dropout)\n",
    "            # logit: [batch_size, class_num]\n",
    "            logit = tf.layers.dense(fc1_dropout, vocab_size, name='logit')\n",
    "            max_prob_word = tf.argmax(logit, axis=1)\n",
    "            max_prob_word = tf.expand_dims(max_prob_word, 1)\n",
    "            generated_words.append(max_prob_word)\n",
    "            word_label = sentence[:, i]\n",
    "            word_mask = mask[:, i]\n",
    "            cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                logits=logit, labels=word_label)\n",
    "            cross_entropys += tf.reduce_sum(tf.multiply(cross_entropy, word_mask))\n",
    "\n",
    "        loss = cross_entropys / tf.reduce_sum(mask)\n",
    "        generated_words = tf.concat(generated_words, 1)\n",
    "\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info(\"variable name: %s\" % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        for grad, var in zip(grads, tvars):\n",
    "            tf.summary.histogram('%s_grad' % (var.name), grad)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step=global_step)\n",
    "\n",
    "    return ((image_feature, sentence, mask, keep_prob),\n",
    "            (loss, generated_words, train_op),\n",
    "            global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 8186\n",
      "INFO:tensorflow:loading features/image_features-0.pickle\n",
      "INFO:tensorflow:loading features/image_features-1.pickle\n",
      "(200, 1536)\n",
      "(200,)\n",
      "INFO:tensorflow:image_feature_dim: 1536\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/kernel:0\n",
      "INFO:tensorflow:variable name: image_feature_embed/dense/bias:0\n",
      "INFO:tensorflow:variable name: fc/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: fc/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: fc/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/logit/kernel:0\n",
      "INFO:tensorflow:variable name: fc/logit/bias:0\n",
      "INFO:tensorflow:Summary name embedding/embeddings:0_grad is illegal; using embedding/embeddings_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/kernel:0_grad is illegal; using image_feature_embed/dense/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name image_feature_embed/dense/bias:0_grad is illegal; using image_feature_embed/dense/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0_grad is illegal; using fc/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0_grad is illegal; using fc/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0_grad is illegal; using fc/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0_grad is illegal; using fc/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/kernel:0_grad is illegal; using fc/fc1/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/bias:0_grad is illegal; using fc/fc1/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logit/kernel:0_grad is illegal; using fc/logit/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/logit/bias:0_grad is illegal; using fc/logit/bias_0_grad instead.\n",
      "INFO:tensorflow:[*] Reading checkpoint ...\n",
      "INFO:tensorflow:Restoring parameters from dir_runs/06091043/lstm-6004\n",
      "INFO:tensorflow:[*] Success Read Checkpoint From lstm-6004\n",
      "INFO:tensorflow:Step:  6005, last_step:    -1, loss: 2.73036, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step:  6106, last_step:  6005, loss: 2.88120, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step:  6207, last_step:  6106, loss: 3.21106, word_accuracy: 0.37500\n",
      "INFO:tensorflow:Step:  6308, last_step:  6207, loss: 3.00210, word_accuracy: 0.32000\n",
      "INFO:tensorflow:Step:  6409, last_step:  6308, loss: 2.89999, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step:  6510, last_step:  6409, loss: 2.75738, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step:  6611, last_step:  6510, loss: 2.64723, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step:  6712, last_step:  6611, loss: 2.49004, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step:  6813, last_step:  6712, loss: 2.47742, word_accuracy: 0.44000\n",
      "INFO:tensorflow:Step:  6914, last_step:  6813, loss: 2.58168, word_accuracy: 0.42000\n",
      "INFO:tensorflow:Step:  7015, last_step:  6914, loss: 2.58836, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step:  7116, last_step:  7015, loss: 2.44436, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step:  7217, last_step:  7116, loss: 2.53433, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step:  7318, last_step:  7217, loss: 2.86989, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step:  7419, last_step:  7318, loss: 2.51220, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step:  7520, last_step:  7419, loss: 2.33821, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step:  7621, last_step:  7520, loss: 2.34539, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step:  7722, last_step:  7621, loss: 1.70728, word_accuracy: 0.56000\n",
      "INFO:tensorflow:Step:  7823, last_step:  7722, loss: 2.47395, word_accuracy: 0.48000\n",
      "INFO:tensorflow:Step:  7924, last_step:  7823, loss: 2.33426, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step: 8005, text classify model saved\n",
      "INFO:tensorflow:Step:  8025, last_step:  7924, loss: 2.21328, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step:  8126, last_step:  8025, loss: 2.74947, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step:  8227, last_step:  8126, loss: 2.51687, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step:  8328, last_step:  8227, loss: 2.11092, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step:  8429, last_step:  8328, loss: 3.09901, word_accuracy: 0.24000\n",
      "INFO:tensorflow:Step:  8530, last_step:  8429, loss: 2.48154, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step:  8631, last_step:  8530, loss: 2.38998, word_accuracy: 0.48000\n",
      "INFO:tensorflow:Step:  8732, last_step:  8631, loss: 2.33848, word_accuracy: 0.48000\n",
      "INFO:tensorflow:Step:  8833, last_step:  8732, loss: 2.00122, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step:  8934, last_step:  8833, loss: 2.44606, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step:  9035, last_step:  8934, loss: 2.30233, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step:  9136, last_step:  9035, loss: 2.43181, word_accuracy: 0.40000\n",
      "INFO:tensorflow:Step:  9237, last_step:  9136, loss: 2.23611, word_accuracy: 0.44000\n",
      "INFO:tensorflow:Step:  9338, last_step:  9237, loss: 2.12563, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step:  9439, last_step:  9338, loss: 2.68735, word_accuracy: 0.30000\n",
      "INFO:tensorflow:Step:  9540, last_step:  9439, loss: 2.07484, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step:  9641, last_step:  9540, loss: 1.99648, word_accuracy: 0.52000\n",
      "INFO:tensorflow:Step:  9742, last_step:  9641, loss: 2.63511, word_accuracy: 0.28000\n",
      "INFO:tensorflow:Step:  9843, last_step:  9742, loss: 1.99376, word_accuracy: 0.48000\n",
      "INFO:tensorflow:Step:  9944, last_step:  9843, loss: 2.08287, word_accuracy: 0.40000\n",
      "INFO:tensorflow:Step: 10005, text classify model saved\n",
      "INFO:tensorflow:Step: 10045, last_step:  9944, loss: 1.95242, word_accuracy: 0.52000\n",
      "INFO:tensorflow:Step: 10146, last_step: 10045, loss: 2.27464, word_accuracy: 0.36000\n",
      "INFO:tensorflow:Step: 10247, last_step: 10146, loss: 2.41728, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step: 10348, last_step: 10247, loss: 1.92484, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step: 10449, last_step: 10348, loss: 1.47314, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step: 10550, last_step: 10449, loss: 2.03951, word_accuracy: 0.54000\n",
      "INFO:tensorflow:Step: 10651, last_step: 10550, loss: 2.14968, word_accuracy: 0.38000\n",
      "INFO:tensorflow:Step: 10752, last_step: 10651, loss: 1.90886, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 10853, last_step: 10752, loss: 2.50900, word_accuracy: 0.34000\n",
      "INFO:tensorflow:Step: 10954, last_step: 10853, loss: 2.01680, word_accuracy: 0.58000\n",
      "INFO:tensorflow:Step: 11055, last_step: 10954, loss: 2.09429, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step: 11156, last_step: 11055, loss: 1.76992, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 11257, last_step: 11156, loss: 2.23250, word_accuracy: 0.40000\n",
      "INFO:tensorflow:Step: 11358, last_step: 11257, loss: 1.75795, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step: 11459, last_step: 11358, loss: 2.57332, word_accuracy: 0.48000\n",
      "INFO:tensorflow:Step: 11560, last_step: 11459, loss: 1.79992, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step: 11661, last_step: 11560, loss: 1.84461, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step: 11762, last_step: 11661, loss: 2.10050, word_accuracy: 0.44000\n",
      "INFO:tensorflow:Step: 11863, last_step: 11762, loss: 1.87353, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step: 11964, last_step: 11863, loss: 1.80889, word_accuracy: 0.56000\n",
      "INFO:tensorflow:Step: 12005, text classify model saved\n",
      "INFO:tensorflow:Step: 12065, last_step: 11964, loss: 2.00405, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step: 12166, last_step: 12065, loss: 1.69318, word_accuracy: 0.58000\n",
      "INFO:tensorflow:Step: 12267, last_step: 12166, loss: 1.88231, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step: 12368, last_step: 12267, loss: 1.64507, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 12469, last_step: 12368, loss: 2.28112, word_accuracy: 0.44000\n",
      "INFO:tensorflow:Step: 12570, last_step: 12469, loss: 1.52283, word_accuracy: 0.54000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step: 12671, last_step: 12570, loss: 1.83333, word_accuracy: 0.58000\n",
      "INFO:tensorflow:Step: 12772, last_step: 12671, loss: 1.62713, word_accuracy: 0.54000\n",
      "INFO:tensorflow:Step: 12873, last_step: 12772, loss: 1.83426, word_accuracy: 0.54000\n",
      "INFO:tensorflow:Step: 12974, last_step: 12873, loss: 1.57090, word_accuracy: 0.58000\n",
      "INFO:tensorflow:Step: 13075, last_step: 12974, loss: 1.86097, word_accuracy: 0.58000\n",
      "INFO:tensorflow:Step: 13176, last_step: 13075, loss: 1.74461, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 13277, last_step: 13176, loss: 2.22008, word_accuracy: 0.46000\n",
      "INFO:tensorflow:Step: 13378, last_step: 13277, loss: 1.77503, word_accuracy: 0.54000\n",
      "INFO:tensorflow:Step: 13479, last_step: 13378, loss: 1.98910, word_accuracy: 0.44000\n",
      "INFO:tensorflow:Step: 13580, last_step: 13479, loss: 2.00450, word_accuracy: 0.54000\n",
      "INFO:tensorflow:Step: 13681, last_step: 13580, loss: 1.70365, word_accuracy: 0.58000\n",
      "INFO:tensorflow:Step: 13782, last_step: 13681, loss: 1.73004, word_accuracy: 0.50000\n",
      "INFO:tensorflow:Step: 13883, last_step: 13782, loss: 1.69530, word_accuracy: 0.56000\n",
      "INFO:tensorflow:Step: 13984, last_step: 13883, loss: 1.43576, word_accuracy: 0.64000\n",
      "INFO:tensorflow:Step: 14005, text classify model saved\n",
      "INFO:tensorflow:Step: 14085, last_step: 13984, loss: 1.63983, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 14186, last_step: 14085, loss: 1.49357, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 14287, last_step: 14186, loss: 1.42418, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 14388, last_step: 14287, loss: 1.68203, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 14489, last_step: 14388, loss: 1.48677, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 14590, last_step: 14489, loss: 1.37120, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 14691, last_step: 14590, loss: 1.76886, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 14792, last_step: 14691, loss: 1.45023, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 14893, last_step: 14792, loss: 1.41545, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 14994, last_step: 14893, loss: 1.37132, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 15095, last_step: 14994, loss: 1.65110, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 15196, last_step: 15095, loss: 1.41212, word_accuracy: 0.64000\n",
      "INFO:tensorflow:Step: 15297, last_step: 15196, loss: 1.47697, word_accuracy: 0.64000\n",
      "INFO:tensorflow:Step: 15398, last_step: 15297, loss: 2.01376, word_accuracy: 0.52000\n",
      "INFO:tensorflow:Step: 15499, last_step: 15398, loss: 1.73534, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step: 15600, last_step: 15499, loss: 1.48059, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 15701, last_step: 15600, loss: 1.65012, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 15802, last_step: 15701, loss: 1.49199, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step: 15903, last_step: 15802, loss: 1.71443, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 16004, last_step: 15903, loss: 1.51402, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 16005, text classify model saved\n",
      "INFO:tensorflow:Step: 16105, last_step: 16004, loss: 1.24301, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 16206, last_step: 16105, loss: 1.38317, word_accuracy: 0.58000\n",
      "INFO:tensorflow:Step: 16307, last_step: 16206, loss: 1.33116, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step: 16408, last_step: 16307, loss: 1.58899, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step: 16509, last_step: 16408, loss: 1.08182, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 16610, last_step: 16509, loss: 1.44313, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 16711, last_step: 16610, loss: 1.41871, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step: 16812, last_step: 16711, loss: 1.06183, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 16913, last_step: 16812, loss: 1.54083, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 17014, last_step: 16913, loss: 1.64675, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 17115, last_step: 17014, loss: 1.64280, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 17216, last_step: 17115, loss: 1.32314, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 17317, last_step: 17216, loss: 1.78011, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step: 17418, last_step: 17317, loss: 1.41042, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 17519, last_step: 17418, loss: 1.38951, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 17620, last_step: 17519, loss: 1.30461, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 17721, last_step: 17620, loss: 1.32414, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 17822, last_step: 17721, loss: 1.28767, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 17923, last_step: 17822, loss: 1.60119, word_accuracy: 0.54000\n",
      "INFO:tensorflow:Step: 18005, text classify model saved\n",
      "INFO:tensorflow:Step: 18024, last_step: 17923, loss: 1.43292, word_accuracy: 0.58000\n",
      "INFO:tensorflow:Step: 18125, last_step: 18024, loss: 1.06677, word_accuracy: 0.78000\n",
      "INFO:tensorflow:Step: 18226, last_step: 18125, loss: 1.52245, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 18327, last_step: 18226, loss: 1.16622, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 18428, last_step: 18327, loss: 1.50000, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 18529, last_step: 18428, loss: 1.33519, word_accuracy: 0.64000\n",
      "INFO:tensorflow:Step: 18630, last_step: 18529, loss: 1.30443, word_accuracy: 0.62000\n",
      "INFO:tensorflow:Step: 18731, last_step: 18630, loss: 1.28592, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 18832, last_step: 18731, loss: 1.09559, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 18933, last_step: 18832, loss: 1.18829, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 19034, last_step: 18933, loss: 1.43396, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 19135, last_step: 19034, loss: 1.31435, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 19236, last_step: 19135, loss: 1.37569, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 19337, last_step: 19236, loss: 1.37254, word_accuracy: 0.64000\n",
      "INFO:tensorflow:Step: 19438, last_step: 19337, loss: 1.60468, word_accuracy: 0.58000\n",
      "INFO:tensorflow:Step: 19539, last_step: 19438, loss: 1.40008, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 19640, last_step: 19539, loss: 1.23996, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 19741, last_step: 19640, loss: 1.28577, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 19842, last_step: 19741, loss: 1.29348, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 19943, last_step: 19842, loss: 1.19994, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 20005, text classify model saved\n",
      "INFO:tensorflow:Step: 20044, last_step: 19943, loss: 1.18693, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 20145, last_step: 20044, loss: 0.96095, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 20246, last_step: 20145, loss: 1.17842, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 20347, last_step: 20246, loss: 1.17000, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 20448, last_step: 20347, loss: 1.35454, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 20549, last_step: 20448, loss: 1.07621, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 20650, last_step: 20549, loss: 1.41551, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 20751, last_step: 20650, loss: 1.09647, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 20852, last_step: 20751, loss: 1.18765, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 20953, last_step: 20852, loss: 1.33931, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 21054, last_step: 20953, loss: 1.33267, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 21155, last_step: 21054, loss: 0.96069, word_accuracy: 0.84000\n",
      "INFO:tensorflow:Step: 21256, last_step: 21155, loss: 1.15677, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 21357, last_step: 21256, loss: 1.34708, word_accuracy: 0.52000\n",
      "INFO:tensorflow:Step: 21458, last_step: 21357, loss: 1.00916, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 21559, last_step: 21458, loss: 1.21000, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 21660, last_step: 21559, loss: 1.04445, word_accuracy: 0.80000\n",
      "INFO:tensorflow:Step: 21761, last_step: 21660, loss: 1.02303, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 21862, last_step: 21761, loss: 1.04379, word_accuracy: 0.78000\n",
      "INFO:tensorflow:Step: 21963, last_step: 21862, loss: 1.26393, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 22005, text classify model saved\n",
      "INFO:tensorflow:Step: 22064, last_step: 21963, loss: 1.39429, word_accuracy: 0.64000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step: 22165, last_step: 22064, loss: 1.30394, word_accuracy: 0.64000\n",
      "INFO:tensorflow:Step: 22266, last_step: 22165, loss: 1.22579, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 22367, last_step: 22266, loss: 1.30650, word_accuracy: 0.60000\n",
      "INFO:tensorflow:Step: 22468, last_step: 22367, loss: 1.11202, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 22569, last_step: 22468, loss: 1.35233, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 22670, last_step: 22569, loss: 1.18168, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 22771, last_step: 22670, loss: 1.21955, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 22872, last_step: 22771, loss: 1.12111, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 22973, last_step: 22872, loss: 1.11485, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 23074, last_step: 22973, loss: 1.25644, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 23175, last_step: 23074, loss: 1.21342, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 23276, last_step: 23175, loss: 1.20090, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 23377, last_step: 23276, loss: 0.92828, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 23478, last_step: 23377, loss: 0.99785, word_accuracy: 0.82000\n",
      "INFO:tensorflow:Step: 23579, last_step: 23478, loss: 1.03657, word_accuracy: 0.80000\n",
      "INFO:tensorflow:Step: 23680, last_step: 23579, loss: 1.08195, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 23781, last_step: 23680, loss: 1.38748, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 23882, last_step: 23781, loss: 1.14932, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 23983, last_step: 23882, loss: 1.25745, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 24005, text classify model saved\n",
      "INFO:tensorflow:Step: 24084, last_step: 23983, loss: 1.23890, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 24185, last_step: 24084, loss: 1.24706, word_accuracy: 0.64000\n",
      "INFO:tensorflow:Step: 24286, last_step: 24185, loss: 0.98583, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 24387, last_step: 24286, loss: 1.05079, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 24488, last_step: 24387, loss: 0.99472, word_accuracy: 0.78000\n",
      "INFO:tensorflow:Step: 24589, last_step: 24488, loss: 1.26493, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 24690, last_step: 24589, loss: 1.06512, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 24791, last_step: 24690, loss: 1.01134, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 24892, last_step: 24791, loss: 0.85123, word_accuracy: 0.80000\n",
      "INFO:tensorflow:Step: 24993, last_step: 24892, loss: 0.91800, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 25094, last_step: 24993, loss: 1.14169, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 25195, last_step: 25094, loss: 1.08666, word_accuracy: 0.82000\n",
      "INFO:tensorflow:Step: 25296, last_step: 25195, loss: 0.93408, word_accuracy: 0.78000\n",
      "INFO:tensorflow:Step: 25397, last_step: 25296, loss: 1.16369, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 25498, last_step: 25397, loss: 1.04270, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 25599, last_step: 25498, loss: 0.98492, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 25700, last_step: 25599, loss: 1.02227, word_accuracy: 0.84000\n",
      "INFO:tensorflow:Step: 25801, last_step: 25700, loss: 1.13629, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 25902, last_step: 25801, loss: 1.13261, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 26003, last_step: 25902, loss: 1.01290, word_accuracy: 0.80000\n",
      "INFO:tensorflow:Step: 26005, text classify model saved\n",
      "INFO:tensorflow:Step: 26104, last_step: 26003, loss: 1.26789, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 26205, last_step: 26104, loss: 0.84666, word_accuracy: 0.78000\n",
      "INFO:tensorflow:Step: 26306, last_step: 26205, loss: 1.27050, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 26407, last_step: 26306, loss: 0.93447, word_accuracy: 0.86000\n",
      "INFO:tensorflow:Step: 26508, last_step: 26407, loss: 1.07629, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 26609, last_step: 26508, loss: 1.02947, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 26710, last_step: 26609, loss: 1.14778, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 26811, last_step: 26710, loss: 0.85036, word_accuracy: 0.82000\n",
      "INFO:tensorflow:Step: 26912, last_step: 26811, loss: 1.01532, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 27013, last_step: 26912, loss: 1.28982, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 27114, last_step: 27013, loss: 1.06410, word_accuracy: 0.70000\n",
      "INFO:tensorflow:Step: 27215, last_step: 27114, loss: 0.82706, word_accuracy: 0.80000\n",
      "INFO:tensorflow:Step: 27316, last_step: 27215, loss: 1.02216, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 27417, last_step: 27316, loss: 1.01119, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 27518, last_step: 27417, loss: 0.98214, word_accuracy: 0.78000\n",
      "INFO:tensorflow:Step: 27619, last_step: 27518, loss: 0.90313, word_accuracy: 0.78000\n",
      "INFO:tensorflow:Step: 27720, last_step: 27619, loss: 1.05547, word_accuracy: 0.66000\n",
      "INFO:tensorflow:Step: 27821, last_step: 27720, loss: 0.99582, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 27922, last_step: 27821, loss: 0.94343, word_accuracy: 0.68000\n",
      "INFO:tensorflow:Step: 28005, text classify model saved\n",
      "INFO:tensorflow:Step: 28023, last_step: 27922, loss: 0.97802, word_accuracy: 0.80000\n",
      "INFO:tensorflow:Step: 28124, last_step: 28023, loss: 0.93379, word_accuracy: 0.80000\n",
      "INFO:tensorflow:Step: 28225, last_step: 28124, loss: 0.93357, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 28326, last_step: 28225, loss: 1.01989, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 28427, last_step: 28326, loss: 1.03066, word_accuracy: 0.72000\n",
      "INFO:tensorflow:Step: 28528, last_step: 28427, loss: 0.95955, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 28629, last_step: 28528, loss: 0.84778, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 28730, last_step: 28629, loss: 0.96248, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 28831, last_step: 28730, loss: 0.98783, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 28932, last_step: 28831, loss: 0.84932, word_accuracy: 0.78000\n",
      "INFO:tensorflow:Step: 29033, last_step: 28932, loss: 0.80212, word_accuracy: 0.80000\n",
      "INFO:tensorflow:Step: 29134, last_step: 29033, loss: 1.04253, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 29235, last_step: 29134, loss: 1.05560, word_accuracy: 0.74000\n",
      "INFO:tensorflow:Step: 29336, last_step: 29235, loss: 0.88680, word_accuracy: 0.82000\n",
      "INFO:tensorflow:Step: 29437, last_step: 29336, loss: 0.90457, word_accuracy: 0.76000\n",
      "INFO:tensorflow:Step: 29538, last_step: 29437, loss: 0.99279, word_accuracy: 0.78000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-942fc6c2fa25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_log\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                 \u001b[0mfetches\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerated_words\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m             \u001b[0mglobal_step_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/zhangyx/workspace/tensorflow_env/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "hps = get_default_params().parse(hp_config)\n",
    "\n",
    "\n",
    "output_dir = output_dir\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "vocab = Vocab(vocab_file, hps.num_vocab_word_threshold)\n",
    "vocab_size = vocab.size()\n",
    "tf.logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "\n",
    "image_name_to_tokens = parse_token_file(token_file)\n",
    "image_name_to_token_ids = convert_token_to_id(image_name_to_tokens, vocab)\n",
    "\n",
    "data = ImageCaptionData(image_name_to_token_ids,\n",
    "                        input_filenamepatten,\n",
    "                        hps.num_timesteps,\n",
    "                        vocab)\n",
    "image_feature_dim = data.image_feature_size()\n",
    "tf.logging.info(\"image_feature_dim: %d\" % image_feature_dim)\n",
    "\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    placeholders, metrics, global_step = create_model(hps, vocab_size, image_feature_dim)\n",
    "    image_feature, sentence, mask, keep_prob = placeholders\n",
    "    loss, generated_words, train_op = metrics\n",
    "    summary_op = tf.summary.merge_all()\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    saver = tf.train.Saver(max_to_keep=10)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(init_op)\n",
    "        writer = tf.summary.FileWriter(output_dir, sess.graph)\n",
    "\n",
    "        tf.logging.info(\"[*] Reading checkpoint ...\")\n",
    "        ckpt = tf.train.get_checkpoint_state(output_dir)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            ckpt_name = os.path.basename(ckpt.model_checkpoint_path)\n",
    "            saver.restore(sess, os.path.join(output_dir, ckpt_name))\n",
    "            tf.logging.info(\"[*] Success Read Checkpoint From %s\" % (ckpt_name))\n",
    "        else:\n",
    "            tf.logging.info(\"[*] Failed load checkpoint\")\n",
    "\n",
    "        last_log_step = -1\n",
    "        last_save_step = -1\n",
    "        for i in range(100000):\n",
    "            batch_image_features, batch_sentence_ids, batch_weights = data.next(hps.batch_size)\n",
    "            input_vals = (batch_image_features, batch_sentence_ids, batch_weights, hps.keep_prob)\n",
    "            feed_dict = dict(zip(placeholders, input_vals))\n",
    "\n",
    "            should_log = last_log_step == -1 or (\n",
    "                global_step_val - last_log_step >= hps.log_frequent)\n",
    "            fetches = [global_step, loss, train_op, summary_op]\n",
    "\n",
    "            if should_log:\n",
    "                fetches += [generated_words]\n",
    "            outputs = sess.run(fetches, feed_dict)\n",
    "            global_step_val, loss_val = outputs[0:2]\n",
    "\n",
    "            if should_log:\n",
    "                summary_str, generated_words_val = outputs[3:]\n",
    "                equal = (generated_words_val == batch_sentence_ids)\n",
    "                weight_equal = equal * batch_weights\n",
    "                accuracy = np.sum(weight_equal) / (np.sum(batch_weights) * 1.0)\n",
    "                writer.add_summary(summary_str, global_step_val)\n",
    "                tf.logging.info(\n",
    "                    'Step: %5d, last_step: %5d, loss: %3.5f, word_accuracy: %3.5f' \n",
    "                    % (global_step_val, last_log_step, loss_val, accuracy))\n",
    "                last_log_step = global_step_val\n",
    "            should_save = last_save_step == -1 or (\n",
    "                global_step_val - last_save_step >= hps.save_frequent)\n",
    "            if should_save:\n",
    "                if last_save_step != -1:\n",
    "                    tf.logging.info(\"Step: %d, text classify model saved\" \n",
    "                                    % (global_step_val))\n",
    "                saver.save(sess, os.path.join(output_dir, \"lstm\"), \n",
    "                           global_step=global_step_val)\n",
    "                last_save_step = global_step_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
