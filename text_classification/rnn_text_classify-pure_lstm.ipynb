{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, filename, num_word_threshold):\n",
    "    self._id_to_word = {}\n",
    "    self._word_to_id = {}\n",
    "    self._unk = -1\n",
    "    self._num_word_threshold = num_word_threshold\n",
    "    self._read_dict(filename)\n",
    "\n",
    "  def _read_dict(self, filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "        word = word.decode('utf-8')\n",
    "        occurence = int(occurence)\n",
    "        if word != '<UNK>' and occurence < self._num_word_threshold:\n",
    "            continue\n",
    "        idx = len(self._id_to_word)\n",
    "        if word == '<UNK>':\n",
    "            self._unk = idx\n",
    "        if idx in self._id_to_word or word in self._word_to_id:\n",
    "            raise Exception('There shouldn\\'t be duplicate word in dict.')\n",
    "        self._word_to_id[word] = idx\n",
    "        self._id_to_word[idx] = word\n",
    "  \n",
    "  @property\n",
    "  def unk(self):\n",
    "    return self._unk\n",
    "\n",
    "  def word_to_id(self, word):\n",
    "    return self._word_to_id.get(word, self._unk)\n",
    "\n",
    "  def size(self):\n",
    "    return len(self._word_to_id)\n",
    "\n",
    "  def encode(self, sentence, data_type):\n",
    "    if data_type == 'word-level':\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "    elif data_type == 'char-level':\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence]\n",
    "    else:\n",
    "        raise Exception('%s is not supported' % (data_type))\n",
    "    return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryVocab(object):\n",
    "  def __init__(self, filename):\n",
    "    self._category_to_id = {}\n",
    "    with open(filename, 'r') as f:\n",
    "      lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "      label, idx = line.strip('\\r\\n').decode('utf-8').split('\\t')\n",
    "      idx = int(idx)\n",
    "      self._category_to_id[label] = idx\n",
    "\n",
    "  def category_to_id(self, category_name):\n",
    "    if not category_name in self._category_to_id:\n",
    "      raise Exception(\"%s is not in our label list.\" % category_name)\n",
    "    return self._category_to_id[category_name]\n",
    "\n",
    "  def get_class_num(self):\n",
    "    return len(self._category_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet:\n",
    "  def __init__(self, filename, vocab, category_vocab, num_timesteps, data_type):\n",
    "    self._vocab = vocab\n",
    "    self._category_vocab = category_vocab\n",
    "    self._inputs = []\n",
    "    self._outputs = []\n",
    "    self._indicator = 0\n",
    "    self._num_timesteps = num_timesteps\n",
    "    self._data_type = data_type\n",
    "    assert self._data_type in ['word-level', 'char-level']\n",
    "    self._parse_file(filename)\n",
    "\n",
    "  def _parse_file(self, filename):\n",
    "    tf.logging.info('Loading data from %s', filename)\n",
    "    with open(filename, 'r') as f:\n",
    "      lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "      label, content = line.strip('\\r\\n').decode('utf-8').split('\\t')\n",
    "      id_label = self._category_vocab.category_to_id(label)\n",
    "      id_words = self._vocab.encode(content, self._data_type)\n",
    "      id_words = id_words[0:self._num_timesteps]\n",
    "      id_words = id_words + [\n",
    "          self._vocab.unk for i in range(self._num_timesteps - len(id_words))]\n",
    "      self._inputs.append(id_words)\n",
    "      self._outputs.append(id_label)\n",
    "\n",
    "    self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "    self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "    self._random_shuffle()\n",
    "\n",
    "  def _random_shuffle(self):\n",
    "    p = np.random.permutation(len(self._inputs))\n",
    "    self._inputs = self._inputs[p]\n",
    "    self._outputs = self._outputs[p]\n",
    "\n",
    "  def next(self, batch_size):\n",
    "    if self._indicator + batch_size > len(self._inputs):\n",
    "        self._random_shuffle()\n",
    "        self._indicator = 0\n",
    "\n",
    "    end_indicator = self._indicator + batch_size\n",
    "    assert end_indicator <= len(self._inputs)\n",
    "\n",
    "    batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "    batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "    self._indicator = end_indicator\n",
    "    return batch_inputs, batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "  return tf.contrib.training.HParams(\n",
    "      num_embedding_nodes=16,\n",
    "      num_timesteps=20,\n",
    "      num_lstm_nodes=[16],\n",
    "      num_lstm_layers=1,\n",
    "      num_fc_nodes=16,\n",
    "      batch_size=100,\n",
    "      cell_type='lstm',\n",
    "      clip_lstm_grads=1.0,\n",
    "      learning_rate=0.001,\n",
    "  )\n",
    "\n",
    "def create_rnn_cell(num_lstm_node, cell_type):\n",
    "  if cell_type == 'lstm':\n",
    "    return tf.contrib.rnn.BasicLSTMCell(num_lstm_node, state_is_tuple=True)\n",
    "  elif cell_type == 'gru':\n",
    "    return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "  else:\n",
    "    raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "  return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    inputs  = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    labels = tf.placeholder(tf.int32, (batch_size,))\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(\n",
    "      tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "\n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "    # Sets up LSTM network.\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    \n",
    "    def _generate_w_b(x_weights_size, h_weights_size, biases_size):\n",
    "        x_w = tf.get_variable(\"x_weights\", x_weights_size)\n",
    "        m_w = tf.get_variable(\"m_weigths\", h_weights_size)\n",
    "        b = tf.get_variable(\"biases\", biases_size, initializer=tf.constant_initializer(0.0))\n",
    "        return x_w, m_w, b\n",
    "        \n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        \"\"\"\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        initial_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        _rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                           embed_inputs,\n",
    "                                           initial_state=initial_state)\n",
    "        last = _rnn_outputs[:, -1, :]\n",
    "        \"\"\"\n",
    "        with tf.variable_scope('inputs'):\n",
    "            ix, ih, ib = _generate_w_b(\n",
    "                x_weights_size = [hps.num_embedding_nodes, hps.num_lstm_nodes[0]],\n",
    "                h_weights_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                biases_size = [1, hps.num_lstm_nodes[0]])\n",
    "        with tf.variable_scope('memory'):\n",
    "            cx, ch, cb = _generate_w_b(\n",
    "                x_weights_size = [hps.num_embedding_nodes, hps.num_lstm_nodes[0]],\n",
    "                h_weights_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                biases_size = [1, hps.num_lstm_nodes[0]])\n",
    "        with tf.variable_scope('forget'):\n",
    "            fx, fh, fb = _generate_w_b(\n",
    "                x_weights_size = [hps.num_embedding_nodes, hps.num_lstm_nodes[0]],\n",
    "                h_weights_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                biases_size = [1, hps.num_lstm_nodes[0]])\n",
    "        with tf.variable_scope('output'):\n",
    "            ox, oh, ob = _generate_w_b(\n",
    "                x_weights_size = [hps.num_embedding_nodes, hps.num_lstm_nodes[0]],\n",
    "                h_weights_size = [hps.num_lstm_nodes[0], hps.num_lstm_nodes[0]],\n",
    "                biases_size = [1, hps.num_lstm_nodes[0]])\n",
    "        state = tf.Variable(tf.zeros([batch_size, hps.num_lstm_nodes[0]]), trainable=False)\n",
    "        h = tf.Variable(tf.zeros([batch_size, hps.num_lstm_nodes[0]]), trainable=False)\n",
    "        for i in range(num_timesteps):\n",
    "            embed_input = embed_inputs[:, i, :]\n",
    "            embed_input = tf.reshape(embed_input, [batch_size, hps.num_embedding_nodes])\n",
    "            forget_gate = tf.sigmoid(\n",
    "                tf.matmul(embed_input, fx) + tf.matmul(h, fh) + fb)\n",
    "            input_gate  = tf.sigmoid(\n",
    "                tf.matmul(embed_input, ix) + tf.matmul(h, ih) + ib)\n",
    "            mid_state = tf.tanh(\n",
    "                tf.matmul(embed_input, cx) + tf.matmul(h, ch) + cb)\n",
    "            state = mid_state * input_gate + state * forget_gate\n",
    "            output_gate = tf.sigmoid(\n",
    "                tf.matmul(embed_input, ox) + tf.matmul(h, oh) + ob)\n",
    "            h = output_gate * tf.tanh(state)\n",
    "        last = h\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "    # Sets up the fully-connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc = tf.layers.dense(last, hps.num_fc_nodes, name='fc1')\n",
    "        fc = tf.contrib.layers.dropout(fc, keep_prob)\n",
    "        fc = tf.nn.relu(fc)\n",
    "        logits = tf.layers.dense(fc, num_classes, name='fc2')\n",
    "\n",
    "    with tf.variable_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits), 1, output_type=tf.int32)\n",
    "        correct_pred = tf.equal(labels, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info(\"variable name: %s\" % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        for grad, var in zip(grads, tvars):\n",
    "            tf.summary.histogram('%s_grad' % (var.name), grad)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step=global_step)\n",
    "\n",
    "    return ((inputs, labels, keep_prob),\n",
    "          (loss, accuracy, train_op),\n",
    "          global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 77323\n",
      "INFO:tensorflow:num_classes: 10\n",
      "INFO:tensorflow:Loading data from cnews_data/word-level/cnews.train.txt\n",
      "INFO:tensorflow:Loading data from cnews_data/word-level/cnews.val.txt\n",
      "INFO:tensorflow:Loading data from cnews_data/word-level/cnews.test.txt\n",
      "WARNING:tensorflow:From <ipython-input-6-3f839f486e57>:87: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: lstm_nn/inputs/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/inputs/m_weigths:0\n",
      "INFO:tensorflow:variable name: lstm_nn/inputs/biases:0\n",
      "INFO:tensorflow:variable name: lstm_nn/memory/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/memory/m_weigths:0\n",
      "INFO:tensorflow:variable name: lstm_nn/memory/biases:0\n",
      "INFO:tensorflow:variable name: lstm_nn/forget/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/forget/m_weigths:0\n",
      "INFO:tensorflow:variable name: lstm_nn/forget/biases:0\n",
      "INFO:tensorflow:variable name: lstm_nn/output/x_weights:0\n",
      "INFO:tensorflow:variable name: lstm_nn/output/m_weigths:0\n",
      "INFO:tensorflow:variable name: lstm_nn/output/biases:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc2/bias:0\n",
      "INFO:tensorflow:Summary name embedding/embeddings:0_grad is illegal; using embedding/embeddings_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/inputs/x_weights:0_grad is illegal; using lstm_nn/inputs/x_weights_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/inputs/m_weigths:0_grad is illegal; using lstm_nn/inputs/m_weigths_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/inputs/biases:0_grad is illegal; using lstm_nn/inputs/biases_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/memory/x_weights:0_grad is illegal; using lstm_nn/memory/x_weights_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/memory/m_weigths:0_grad is illegal; using lstm_nn/memory/m_weigths_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/memory/biases:0_grad is illegal; using lstm_nn/memory/biases_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/forget/x_weights:0_grad is illegal; using lstm_nn/forget/x_weights_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/forget/m_weigths:0_grad is illegal; using lstm_nn/forget/m_weigths_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/forget/biases:0_grad is illegal; using lstm_nn/forget/biases_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/output/x_weights:0_grad is illegal; using lstm_nn/output/x_weights_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/output/m_weigths:0_grad is illegal; using lstm_nn/output/m_weigths_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/output/biases:0_grad is illegal; using lstm_nn/output/biases_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/kernel:0_grad is illegal; using fc/fc1/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/bias:0_grad is illegal; using fc/fc1/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc2/kernel:0_grad is illegal; using fc/fc2/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc2/bias:0_grad is illegal; using fc/fc2/bias_0_grad instead.\n"
     ]
    }
   ],
   "source": [
    "word_level_train_file = 'cnews_data/word-level/cnews.train.txt'\n",
    "word_level_val_file = 'cnews_data/word-level/cnews.val.txt'\n",
    "word_level_test_file  = 'cnews_data/word-level/cnews.test.txt'\n",
    "word_level_vocab_file = 'cnews_data/word-level/cnews.vocab.txt'\n",
    "word_level_category_file = 'cnews_data/word-level/cnews.category.txt'\n",
    "word_level_output_folder = 'cnews_data/word-level/dir_runs'\n",
    "\n",
    "char_level_train_file = 'cnews_data/char-level/cnews.train.txt'\n",
    "char_level_val_file = 'cnews_data/char-level/cnews.val.txt'\n",
    "char_level_test_file  = 'cnews_data/char-level/cnews.test.txt'\n",
    "char_level_vocab_file = 'cnews_data/char-level/cnews.vocab.txt'\n",
    "char_level_category_file = 'cnews_data/char-level/cnews.category.txt'\n",
    "char_level_output_folder = 'cnews_data/char-level/dir_runs'\n",
    "\n",
    "data_type = 'word-level'\n",
    "num_word_threshold = 10\n",
    "\n",
    "if data_type == 'word-level':\n",
    "    train_file = word_level_train_file\n",
    "    val_file = word_level_val_file\n",
    "    test_file = word_level_test_file\n",
    "    vocab_file = word_level_vocab_file\n",
    "    category_file = word_level_category_file\n",
    "    output_folder = word_level_output_folder\n",
    "elif data_type == 'char-level':\n",
    "    train_file = char_level_train_file\n",
    "    val_file = char_level_val_file\n",
    "    test_file = char_level_test_file\n",
    "    vocab_file = char_level_vocab_file\n",
    "    category_file = char_level_category_file\n",
    "    output_folder = char_level_output_folder\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "hps = get_default_params()\n",
    "vocab = Vocab(vocab_file, num_word_threshold)\n",
    "category_vocab = CategoryVocab(category_file)\n",
    "\n",
    "vocab_size = vocab.size()\n",
    "num_classes = category_vocab.get_class_num()\n",
    "tf.logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "tf.logging.info(\"num_classes: %d\" % num_classes)\n",
    "\n",
    "train_dataset = TextDataSet(train_file, vocab, category_vocab, hps.num_timesteps, data_type)\n",
    "val_dataset = TextDataSet(val_file, vocab, category_vocab, hps.num_timesteps, data_type)\n",
    "test_dataset = TextDataSet(test_file, vocab, category_vocab, hps.num_timesteps, data_type)\n",
    "\n",
    "placeholders, metrics, global_step = create_model(hps, vocab_size, num_classes)\n",
    "\n",
    "loss, accuracy, train_op = metrics\n",
    "summary_op = tf.summary.merge_all()\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "train_keep_rate_for_dropout = 0.8\n",
    "test_keep_rate_for_dropout = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:   100, loss: 2.18151, accuracy: 0.13000\n",
      "INFO:tensorflow:Step:   200, loss: 2.02622, accuracy: 0.22000\n",
      "INFO:tensorflow:Step:   300, loss: 1.88564, accuracy: 0.27000\n",
      "INFO:tensorflow:Step:   400, loss: 1.66959, accuracy: 0.39000\n",
      "INFO:tensorflow:Step:   500, loss: 1.55358, accuracy: 0.37000\n",
      "INFO:tensorflow:Step:   600, loss: 1.65177, accuracy: 0.41000\n",
      "INFO:tensorflow:Step:   700, loss: 1.39265, accuracy: 0.46000\n",
      "INFO:tensorflow:Step:   800, loss: 1.20482, accuracy: 0.49000\n",
      "INFO:tensorflow:Step:   900, loss: 1.37823, accuracy: 0.45000\n",
      "INFO:tensorflow:Step:  1000, loss: 1.11018, accuracy: 0.56000\n",
      "INFO:tensorflow:Step:  1100, loss: 1.24467, accuracy: 0.62000\n",
      "INFO:tensorflow:Step:  1200, loss: 1.20474, accuracy: 0.58000\n",
      "INFO:tensorflow:Step:  1300, loss: 0.88488, accuracy: 0.67000\n",
      "INFO:tensorflow:Step:  1400, loss: 0.79592, accuracy: 0.67000\n",
      "INFO:tensorflow:Step:  1500, loss: 1.06686, accuracy: 0.66000\n",
      "INFO:tensorflow:Step:  1600, loss: 0.78043, accuracy: 0.66000\n",
      "INFO:tensorflow:Step:  1700, loss: 0.72545, accuracy: 0.74000\n",
      "INFO:tensorflow:Step:  1800, loss: 0.72936, accuracy: 0.73000\n",
      "INFO:tensorflow:Step:  1900, loss: 0.91864, accuracy: 0.71000\n",
      "INFO:tensorflow:Step:  2000, loss: 0.85614, accuracy: 0.70000\n",
      "INFO:tensorflow:Step:  2100, loss: 0.50649, accuracy: 0.83000\n",
      "INFO:tensorflow:Step:  2200, loss: 0.50801, accuracy: 0.82000\n",
      "INFO:tensorflow:Step:  2300, loss: 0.68146, accuracy: 0.80000\n",
      "INFO:tensorflow:Step:  2400, loss: 0.56618, accuracy: 0.80000\n",
      "INFO:tensorflow:Step:  2500, loss: 0.55892, accuracy: 0.83000\n",
      "INFO:tensorflow:Step:  2600, loss: 0.35711, accuracy: 0.84000\n",
      "INFO:tensorflow:Step:  2700, loss: 0.48868, accuracy: 0.86000\n",
      "INFO:tensorflow:Step:  2800, loss: 0.41626, accuracy: 0.85000\n",
      "INFO:tensorflow:Step:  2900, loss: 0.37041, accuracy: 0.87000\n",
      "INFO:tensorflow:Step:  3000, loss: 0.76956, accuracy: 0.75000\n",
      "INFO:tensorflow:Step:  3100, loss: 0.46988, accuracy: 0.88000\n",
      "INFO:tensorflow:Step:  3200, loss: 0.47187, accuracy: 0.84000\n",
      "INFO:tensorflow:Step:  3300, loss: 0.30866, accuracy: 0.91000\n",
      "INFO:tensorflow:Step:  3400, loss: 0.31499, accuracy: 0.88000\n",
      "INFO:tensorflow:Step:  3500, loss: 0.30903, accuracy: 0.89000\n",
      "INFO:tensorflow:Step:  3600, loss: 0.25523, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  3700, loss: 0.52423, accuracy: 0.89000\n",
      "INFO:tensorflow:Step:  3800, loss: 0.27282, accuracy: 0.91000\n",
      "INFO:tensorflow:Step:  3900, loss: 0.19940, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  4000, loss: 0.32260, accuracy: 0.91000\n",
      "INFO:tensorflow:Step:  4100, loss: 0.40282, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  4200, loss: 0.22345, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  4300, loss: 0.25253, accuracy: 0.92000\n",
      "INFO:tensorflow:Step:  4400, loss: 0.17379, accuracy: 0.95000\n",
      "INFO:tensorflow:Step:  4500, loss: 0.15233, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  4600, loss: 0.09429, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  4700, loss: 0.25643, accuracy: 0.95000\n",
      "INFO:tensorflow:Step:  4800, loss: 0.20224, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  4900, loss: 0.22137, accuracy: 0.90000\n",
      "INFO:tensorflow:Step:  5000, loss: 0.30146, accuracy: 0.94000\n"
     ]
    }
   ],
   "source": [
    "num_train_iters = 5000\n",
    "num_test_iters = 100\n",
    "num_val_iters = 50\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_folder, sess.graph)\n",
    "    for i in range(num_train_iters):\n",
    "        batch_inputs, batch_labels = train_dataset.next(hps.batch_size)\n",
    "        input_vals = (batch_inputs, batch_labels, train_keep_rate_for_dropout)\n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op, summary_op]\n",
    "        outputs = sess.run(fetches, feed_dict)\n",
    "\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if global_step_val % 100 == 0:\n",
    "            tf.logging.info('Step: %5d, loss: %3.5f, accuracy: %4.5f'\n",
    "                            % (global_step_val, loss_val, accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "word-level: filter=100 vocab_size=17067\n",
    "char-level: filter=100 vocab_size=3583\n",
    "        word    char\n",
    "Train   99.7%   98.9%\n",
    "Valid   92.7%   94.4%\n",
    "Test    93.2%   95%\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
