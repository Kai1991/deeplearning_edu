{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchyAttentionNetwork:\n",
    "    def __init__(self, W_embedding):\n",
    "        with tf.name_scope('Inputs'):\n",
    "            self._X1_inputs = tf.placeholder(\n",
    "                tf.int64, [None, self.title_len], name='X1_inputs')\n",
    "            self._X2_inputs = tf.placeholder(\n",
    "                tf.int64, [None, self.doc_len * self.sent_len], name='X2_inputs')\n",
    "            self._y_inputs = tf.placeholder(\n",
    "                tf.float32, [self.n_class], name='y_input')\n",
    "\n",
    "        with tf.variable_scope('embedding'):\n",
    "            self.embedding = tf.get_variable(\n",
    "                name='embedding', shape=W_embedding.shape,\n",
    "                initializer=tf.constant_initializer(W_embedding), trainable=True)\n",
    "        self.embedding_size = W_embedding.shape[1]\n",
    "\n",
    "        with tf.variable_scope('bigru_text'):\n",
    "            output_title = self.bigru_inference(self._X1_inputs)\n",
    "\n",
    "        with tf.variable_scope('han_content'):\n",
    "            output_content = self.han_inference(self._X2_inputs)\n",
    "\n",
    "        with tf.variable_scope('fc-bn-layer'):\n",
    "            output = tf.concat([output_title, output_content], axis=1)\n",
    "            W_fc = self.weight_variable(\n",
    "                [self.hidden_size * 4, self.fc_hidden_size], name='Weight_fc')\n",
    "            h_fc = tf.matmul(output, W_fc, name='h_fc')\n",
    "            self.fc_relu = tf.nn.relu(h_fc, name=\"relu\")\n",
    "            fc_drop = tf.nn.dropout(self.fc_relu, self.keep_prob)\n",
    "\n",
    "        with tf.variable_scope('out_layer'):\n",
    "            W_out = self.weight_variable(\n",
    "                [self.fc_hidden_size, self.n_class], name='Weight_out')\n",
    "            b_out = self.bias_variable([self.n_class], name='bias_out')\n",
    "            self._y_pred = tf.nn.xw_plus_b(fc_drop, W_out, b_out, name='y_pred')\n",
    "\n",
    "        with tf.name_scope('loss'):\n",
    "            self._loss = tf.reduce_mean(\n",
    "                tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "                    logits=self._y_pred, labels=self._y_inputs))\n",
    "    \n",
    "    def gru_cell(self):\n",
    "        with tf.name_scope('gru_cell'):\n",
    "            cell = rnn.GRUCell(self.hidden_size, reuse=tf.get_variable_scope().reuse)\n",
    "        return cell\n",
    "    \n",
    "    def bi_gru(self, inputs, seg_num):\n",
    "        cells_fw = [self.gru_cell() for _ in range(self.n_layer)]\n",
    "        cells_bw = [self.gru_cell() for _ in range(self.n_layer)]\n",
    "        initial_states_fw = [cell_fw.zero_state(seg_num, tf.float32) for cell_fw in cells_fw]\n",
    "        initial_states_bw = [cell_bw.zero_state(seg_num, tf.float32) for cell_bw in cells_bw]\n",
    "        outputs, _, _ = rnn.stack_bidirectional_dynamic_rnn(\n",
    "            cells_fw, cells_bw, inputs,\n",
    "            initial_states_fw = initial_states_fw, \n",
    "            initial_states_bw = initial_states_bw,\n",
    "            dtype=tf.float32)\n",
    "        # output_size: [seg_num, timesteps, hidden_size * 2]\n",
    "        return outputs\n",
    "    \n",
    "    def attention(self, inputs, output_size,\n",
    "                                initializer=layers.xavier_initializer(),\n",
    "                                activation_fn=tf.tanh, scope=None):\n",
    "        with tf.variable_scope(scope or 'attention') as scope:\n",
    "            # attention: [output_size]\n",
    "            attention_context_vector = tf.get_variable(\n",
    "                name='attention_context_vector',\n",
    "                shape=[output_size],\n",
    "                initializer=initializer,\n",
    "                dtype=tf.float32)\n",
    "            \n",
    "            # [seg_num, timesteps, input_size] -> [seg_num, timesteps, output_size]\n",
    "            input_projection = layers.fully_connected(\n",
    "                inputs, output_size, activation_fn=activation_fn, scope=scope)\n",
    "            \n",
    "            # [seg_num, timesteps, output_size] -> [seg_num, timesteps]\n",
    "            vector_attn = tf.reduce_sum(\n",
    "                tf.multiply(input_projection, attention_context_vector),\n",
    "                axis=2,\n",
    "                keep_dims=True)\n",
    "            \n",
    "            attention_weights = tf.nn.softmax(vector_attn, dim=1)\n",
    "\n",
    "            # inputs: [seg_num, timesteps, input_size]\n",
    "            # attention_weights: [seg_num, timesteps]\n",
    "            weighted_projection = tf.multiply(inputs, attention_weights)\n",
    "            \n",
    "            # outputs: [seg_num, input_size]\n",
    "            outputs = tf.reduce_sum(weighted_projection, axis=1)\n",
    "            return outputs\n",
    "    def han_inference(self, X_inputs):\n",
    "        # inputs: [batch_size, doc_len, sent_len, embedding_size]\n",
    "        inputs = tf.nn.embedding_lookup(self.embedding, X_inputs)\n",
    "        \n",
    "        # sent_inputs [batch_size * doc_len, sent_len, embedding_size]\n",
    "        sent_inputs = tf.reshape(inputs,\n",
    "                                 [self.batch_size*self.doc_len,\n",
    "                                  self.sent_len,\n",
    "                                  self.embedding_size])\n",
    "        \n",
    "        with tf.variable_scope('sentence_encoder'):\n",
    "            sent_outputs = self.bi_gru(sent_inputs, seg_num=self.batch_size*self.doc_len)\n",
    "            \n",
    "            # sent_attn_outputs: [seg_num, hidden_size*2]\n",
    "            sent_attn_outputs = self.attention(sent_outputs, self.hidden_size*2)\n",
    "        with tf.variable_scope('doc_encoder'): \n",
    "            # doc_inputs: [batch_size, doc_len, hidden_size*2]\n",
    "            doc_inputs = tf.reshape(sent_attn_outputs, [self.batch_size, self.doc_len, self.hidden_size*2])\n",
    "            \n",
    "            # doc_outputs: [batch_size, doc_len, hidden_size*2]\n",
    "            doc_outputs = self.bi_gru(doc_inputs, self.batch_size)\n",
    "            \n",
    "            # doc_attn_outputs: [batch_size, hidden_size * 2]\n",
    "            doc_attn_outputs = self.attention(doc_outputs, self.hidden_size*2)\n",
    "        return doc_attn_outputs\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
