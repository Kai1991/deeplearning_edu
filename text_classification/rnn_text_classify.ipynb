{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, filename, num_word_threshold):\n",
    "    self._id_to_word = {}\n",
    "    self._word_to_id = {}\n",
    "    self._unk = -1\n",
    "    self._num_word_threshold = num_word_threshold\n",
    "    self._read_dict(filename)\n",
    "\n",
    "  def _read_dict(self, filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        word, occurence = line.strip('\\r\\n').split('\\t')\n",
    "        word = word.decode('utf-8')\n",
    "        occurence = int(occurence)\n",
    "        if word != '<UNK>' and occurence < self._num_word_threshold:\n",
    "            continue\n",
    "        idx = len(self._id_to_word)\n",
    "        if word == '<UNK>':\n",
    "            self._unk = idx\n",
    "        if idx in self._id_to_word or word in self._word_to_id:\n",
    "            raise Exception('There shouldn\\'t be duplicate word in dict.')\n",
    "        self._word_to_id[word] = idx\n",
    "        self._id_to_word[idx] = word\n",
    "  \n",
    "  @property\n",
    "  def unk(self):\n",
    "    return self._unk\n",
    "\n",
    "  def word_to_id(self, word):\n",
    "    return self._word_to_id.get(word, self._unk)\n",
    "\n",
    "  def size(self):\n",
    "    return len(self._word_to_id)\n",
    "\n",
    "  def encode(self, sentence, data_type):\n",
    "    if data_type == 'word-level':\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence.split(' ')]\n",
    "    elif data_type == 'char-level':\n",
    "        word_ids = [self.word_to_id(cur_word) for cur_word in sentence]\n",
    "    else:\n",
    "        raise Exception('%s is not supported' % (data_type))\n",
    "    return word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryVocab(object):\n",
    "  def __init__(self, filename):\n",
    "    self._category_to_id = {}\n",
    "    with open(filename, 'r') as f:\n",
    "      lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "      label, idx = line.strip('\\r\\n').decode('utf-8').split('\\t')\n",
    "      idx = int(idx)\n",
    "      self._category_to_id[label] = idx\n",
    "\n",
    "  def category_to_id(self, category_name):\n",
    "    if not category_name in self._category_to_id:\n",
    "      raise Exception(\"%s is not in our label list.\" % category_name)\n",
    "    return self._category_to_id[category_name]\n",
    "\n",
    "  def get_class_num(self):\n",
    "    return len(self._category_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataSet:\n",
    "  def __init__(self, filename, vocab, category_vocab, num_timesteps, data_type):\n",
    "    self._vocab = vocab\n",
    "    self._category_vocab = category_vocab\n",
    "    self._inputs = []\n",
    "    self._outputs = []\n",
    "    self._indicator = 0\n",
    "    self._num_timesteps = num_timesteps\n",
    "    self._data_type = data_type\n",
    "    assert self._data_type in ['word-level', 'char-level']\n",
    "    self._parse_file(filename)\n",
    "\n",
    "  def _parse_file(self, filename):\n",
    "    tf.logging.info('Loading data from %s', filename)\n",
    "    with open(filename, 'r') as f:\n",
    "      lines = f.readlines()\n",
    "\n",
    "    for line in lines:\n",
    "      label, content = line.strip('\\r\\n').decode('utf-8').split('\\t')\n",
    "      id_label = self._category_vocab.category_to_id(label)\n",
    "      id_words = self._vocab.encode(content, self._data_type)\n",
    "      id_words = id_words[0:self._num_timesteps]\n",
    "      id_words = id_words + [\n",
    "          self._vocab.unk for i in range(self._num_timesteps - len(id_words))]\n",
    "      self._inputs.append(id_words)\n",
    "      self._outputs.append(id_label)\n",
    "\n",
    "    self._inputs = np.asarray(self._inputs, dtype=np.int32)\n",
    "    self._outputs = np.asarray(self._outputs, dtype=np.int32)\n",
    "    self._random_shuffle()\n",
    "\n",
    "  def _random_shuffle(self):\n",
    "    p = np.random.permutation(len(self._inputs))\n",
    "    self._inputs = self._inputs[p]\n",
    "    self._outputs = self._outputs[p]\n",
    "\n",
    "  def next(self, batch_size):\n",
    "    if self._indicator + batch_size > len(self._inputs):\n",
    "        self._random_shuffle()\n",
    "        self._indicator = 0\n",
    "\n",
    "    end_indicator = self._indicator + batch_size\n",
    "    assert end_indicator <= len(self._inputs)\n",
    "\n",
    "    batch_inputs = self._inputs[self._indicator: end_indicator]\n",
    "    batch_outputs = self._outputs[self._indicator: end_indicator]\n",
    "    self._indicator = end_indicator\n",
    "    return batch_inputs, batch_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_params():\n",
    "  return tf.contrib.training.HParams(\n",
    "      num_embedding_nodes=16,\n",
    "      num_timesteps=600,\n",
    "      num_lstm_nodes=[32, 32],\n",
    "      num_lstm_layers=2,\n",
    "      num_fc_nodes=32,\n",
    "      batch_size=100,\n",
    "      cell_type='lstm',\n",
    "      clip_lstm_grads=1.0,\n",
    "      learning_rate=0.001,\n",
    "  )\n",
    "\n",
    "def create_rnn_cell(num_lstm_node, cell_type):\n",
    "  if cell_type == 'lstm':\n",
    "    return tf.contrib.rnn.BasicLSTMCell(num_lstm_node, state_is_tuple=True)\n",
    "  elif cell_type == 'gru':\n",
    "    return tf.contrib.rnn.GRUCell(hidden_dim)\n",
    "  else:\n",
    "    raise Exception(\"%s has not been supported\" % cell_type)\n",
    "\n",
    "def dropout(cell, keep_prob):\n",
    "  return tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(hps, vocab_size, num_classes):\n",
    "    num_timesteps = hps.num_timesteps\n",
    "    batch_size = hps.batch_size\n",
    "\n",
    "    inputs  = tf.placeholder(tf.int32, (batch_size, num_timesteps))\n",
    "    labels = tf.placeholder(tf.int32, (batch_size,))\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "    global_step = tf.Variable(\n",
    "      tf.zeros([], tf.int64), name='global_step', trainable=False)\n",
    "\n",
    "    # Sets up the embedding layer.\n",
    "    embedding_initializer = tf.random_uniform_initializer(-1.0, 1.0)\n",
    "    with tf.variable_scope('embedding', initializer=embedding_initializer):\n",
    "        embeddings = tf.get_variable(\n",
    "            'embeddings',\n",
    "            [vocab_size, hps.num_embedding_nodes],\n",
    "            tf.float32)\n",
    "        embed_inputs = tf.nn.embedding_lookup(embeddings, inputs)\n",
    "\n",
    "    # Sets up LSTM network.\n",
    "    scale = 1.0 / math.sqrt(hps.num_embedding_nodes + hps.num_lstm_nodes[-1]) / 3.0\n",
    "    lstm_init = tf.random_uniform_initializer(-scale, scale)\n",
    "    with tf.variable_scope('lstm_nn', initializer=lstm_init):\n",
    "        cells = []\n",
    "        for i in range(hps.num_lstm_layers):\n",
    "            cell = create_rnn_cell(hps.num_lstm_nodes[i], hps.cell_type)\n",
    "            cell = dropout(cell, keep_prob)\n",
    "            cells.append(cell)\n",
    "        cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
    "        initial_state = cell.zero_state(hps.batch_size, tf.float32)\n",
    "        _rnn_outputs, _ = tf.nn.dynamic_rnn(cell,\n",
    "                                           embed_inputs,\n",
    "                                           initial_state=initial_state)\n",
    "        last = _rnn_outputs[:, -1, :]\n",
    "\n",
    "    # Sets up the fully-connected layer.\n",
    "    fc_init = tf.uniform_unit_scaling_initializer(factor=1.0)\n",
    "    with tf.variable_scope('fc', initializer=fc_init):\n",
    "        fc = tf.layers.dense(last, hps.num_fc_nodes, name='fc1')\n",
    "        fc = tf.contrib.layers.dropout(fc, keep_prob)\n",
    "        fc = tf.nn.relu(fc)\n",
    "        logits = tf.layers.dense(fc, num_classes, name='fc2')\n",
    "\n",
    "    with tf.variable_scope('metrics'):\n",
    "        softmax_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "        loss = tf.reduce_mean(softmax_loss)\n",
    "        y_pred = tf.argmax(tf.nn.softmax(logits), 1, output_type=tf.int32)\n",
    "        correct_pred = tf.equal(labels, y_pred)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "    with tf.variable_scope('train_op'):\n",
    "        tvars = tf.trainable_variables()\n",
    "        for var in tvars:\n",
    "            tf.logging.info(\"variable name: %s\" % (var.name))\n",
    "        grads, _ = tf.clip_by_global_norm(\n",
    "            tf.gradients(loss, tvars), hps.clip_lstm_grads)\n",
    "        for grad, var in zip(grads, tvars):\n",
    "            tf.summary.histogram('%s_grad' % (var.name), grad)\n",
    "        optimizer = tf.train.AdamOptimizer(hps.learning_rate)\n",
    "        train_op = optimizer.apply_gradients(zip(grads, tvars),\n",
    "                                             global_step=global_step)\n",
    "\n",
    "    return ((inputs, labels, keep_prob),\n",
    "          (loss, accuracy, train_op),\n",
    "          global_step)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:vocab_size: 77323\n",
      "INFO:tensorflow:num_classes: 10\n",
      "INFO:tensorflow:Loading data from cnews_data/word-level/cnews.train.txt\n",
      "INFO:tensorflow:Loading data from cnews_data/word-level/cnews.val.txt\n",
      "INFO:tensorflow:Loading data from cnews_data/word-level/cnews.test.txt\n",
      "WARNING:tensorflow:From <ipython-input-6-625e8aa7c715>:38: __init__ (from tensorflow.python.ops.init_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.initializers.variance_scaling instead with distribution=uniform to get equivalent behavior.\n",
      "INFO:tensorflow:variable name: embedding/embeddings:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0\n",
      "INFO:tensorflow:variable name: lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc1/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc1/bias:0\n",
      "INFO:tensorflow:variable name: fc/fc2/kernel:0\n",
      "INFO:tensorflow:variable name: fc/fc2/bias:0\n",
      "INFO:tensorflow:Summary name embedding/embeddings:0_grad is illegal; using embedding/embeddings_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0_grad is illegal; using lstm_nn/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/kernel:0_grad is illegal; using fc/fc1/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc1/bias:0_grad is illegal; using fc/fc1/bias_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc2/kernel:0_grad is illegal; using fc/fc2/kernel_0_grad instead.\n",
      "INFO:tensorflow:Summary name fc/fc2/bias:0_grad is illegal; using fc/fc2/bias_0_grad instead.\n"
     ]
    }
   ],
   "source": [
    "word_level_train_file = 'cnews_data/word-level/cnews.train.txt'\n",
    "word_level_val_file = 'cnews_data/word-level/cnews.val.txt'\n",
    "word_level_test_file  = 'cnews_data/word-level/cnews.test.txt'\n",
    "word_level_vocab_file = 'cnews_data/word-level/cnews.vocab.txt'\n",
    "word_level_category_file = 'cnews_data/word-level/cnews.category.txt'\n",
    "word_level_output_folder = 'cnews_data/word-level/dir_runs'\n",
    "\n",
    "char_level_train_file = 'cnews_data/char-level/cnews.train.txt'\n",
    "char_level_val_file = 'cnews_data/char-level/cnews.val.txt'\n",
    "char_level_test_file  = 'cnews_data/char-level/cnews.test.txt'\n",
    "char_level_vocab_file = 'cnews_data/char-level/cnews.vocab.txt'\n",
    "char_level_category_file = 'cnews_data/char-level/cnews.category.txt'\n",
    "char_level_output_folder = 'cnews_data/char-level/dir_runs'\n",
    "\n",
    "data_type = 'word-level'\n",
    "num_word_threshold = 10\n",
    "\n",
    "if data_type == 'word-level':\n",
    "    train_file = word_level_train_file\n",
    "    val_file = word_level_val_file\n",
    "    test_file = word_level_test_file\n",
    "    vocab_file = word_level_vocab_file\n",
    "    category_file = word_level_category_file\n",
    "    output_folder = word_level_output_folder\n",
    "elif data_type == 'char-level':\n",
    "    train_file = char_level_train_file\n",
    "    val_file = char_level_val_file\n",
    "    test_file = char_level_test_file\n",
    "    vocab_file = char_level_vocab_file\n",
    "    category_file = char_level_category_file\n",
    "    output_folder = char_level_output_folder\n",
    "\n",
    "if not os.path.exists(output_folder):\n",
    "    os.mkdir(output_folder)\n",
    "\n",
    "hps = get_default_params()\n",
    "vocab = Vocab(vocab_file, num_word_threshold)\n",
    "category_vocab = CategoryVocab(category_file)\n",
    "\n",
    "vocab_size = vocab.size()\n",
    "num_classes = category_vocab.get_class_num()\n",
    "tf.logging.info(\"vocab_size: %d\" % vocab_size)\n",
    "tf.logging.info(\"num_classes: %d\" % num_classes)\n",
    "\n",
    "train_dataset = TextDataSet(train_file, vocab, category_vocab, hps.num_timesteps, data_type)\n",
    "val_dataset = TextDataSet(val_file, vocab, category_vocab, hps.num_timesteps, data_type)\n",
    "test_dataset = TextDataSet(test_file, vocab, category_vocab, hps.num_timesteps, data_type)\n",
    "\n",
    "placeholders, metrics, global_step = create_model(hps, vocab_size, num_classes)\n",
    "\n",
    "loss, accuracy, train_op = metrics\n",
    "summary_op = tf.summary.merge_all()\n",
    "init_op = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver(max_to_keep=10)\n",
    "\n",
    "train_keep_rate_for_dropout = 0.8\n",
    "test_keep_rate_for_dropout = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Step:   100, loss: 2.22502, accuracy: 0.15000\n",
      "INFO:tensorflow:Step:   200, loss: 2.19878, accuracy: 0.08000\n",
      "INFO:tensorflow:Step:   300, loss: 2.23281, accuracy: 0.11000\n",
      "INFO:tensorflow:Step:   400, loss: 2.13852, accuracy: 0.19000\n",
      "INFO:tensorflow:Step:   500, loss: 2.27273, accuracy: 0.10000\n",
      "INFO:tensorflow:Step:   600, loss: 2.24877, accuracy: 0.07000\n",
      "INFO:tensorflow:Step:   700, loss: 2.21698, accuracy: 0.10000\n",
      "INFO:tensorflow:Step:   800, loss: 2.10033, accuracy: 0.15000\n",
      "INFO:tensorflow:Step:   900, loss: 2.00499, accuracy: 0.29000\n",
      "INFO:tensorflow:Step:  1000, loss: 1.87917, accuracy: 0.19000\n",
      "INFO:tensorflow:Step:  1100, loss: 1.83067, accuracy: 0.16000\n",
      "INFO:tensorflow:Step:  1200, loss: 1.85928, accuracy: 0.18000\n",
      "INFO:tensorflow:Step:  1300, loss: 1.77081, accuracy: 0.18000\n",
      "INFO:tensorflow:Step:  1400, loss: 2.13553, accuracy: 0.17000\n",
      "INFO:tensorflow:Step:  1500, loss: 1.67256, accuracy: 0.31000\n",
      "INFO:tensorflow:Step:  1600, loss: 1.64716, accuracy: 0.31000\n",
      "INFO:tensorflow:Step:  1700, loss: 1.58916, accuracy: 0.32000\n",
      "INFO:tensorflow:Step:  1800, loss: 1.48263, accuracy: 0.35000\n",
      "INFO:tensorflow:Step:  1900, loss: 1.38134, accuracy: 0.38000\n",
      "INFO:tensorflow:Step:  2000, loss: 1.31935, accuracy: 0.31000\n",
      "INFO:tensorflow:Step:  2100, loss: 1.28822, accuracy: 0.45000\n",
      "INFO:tensorflow:Step:  2200, loss: 1.31994, accuracy: 0.43000\n",
      "INFO:tensorflow:Step:  2300, loss: 1.24728, accuracy: 0.51000\n",
      "INFO:tensorflow:Step:  2400, loss: 1.33678, accuracy: 0.54000\n",
      "INFO:tensorflow:Step:  2500, loss: 1.22649, accuracy: 0.47000\n",
      "INFO:tensorflow:Step:  2600, loss: 1.21660, accuracy: 0.59000\n",
      "INFO:tensorflow:Step:  2700, loss: 1.01927, accuracy: 0.57000\n",
      "INFO:tensorflow:Step:  2800, loss: 1.15214, accuracy: 0.59000\n",
      "INFO:tensorflow:Step:  2900, loss: 1.06535, accuracy: 0.51000\n",
      "INFO:tensorflow:Step:  3000, loss: 0.92279, accuracy: 0.59000\n",
      "INFO:tensorflow:Step:  3100, loss: 0.98204, accuracy: 0.63000\n",
      "INFO:tensorflow:Step:  3200, loss: 0.87548, accuracy: 0.63000\n",
      "INFO:tensorflow:Step:  3300, loss: 0.85879, accuracy: 0.67000\n",
      "INFO:tensorflow:Step:  3400, loss: 0.85888, accuracy: 0.64000\n",
      "INFO:tensorflow:Step:  3500, loss: 0.62996, accuracy: 0.77000\n",
      "INFO:tensorflow:Step:  3600, loss: 0.60917, accuracy: 0.75000\n",
      "INFO:tensorflow:Step:  3700, loss: 0.78980, accuracy: 0.76000\n",
      "INFO:tensorflow:Step:  3800, loss: 0.63573, accuracy: 0.76000\n",
      "INFO:tensorflow:Step:  3900, loss: 0.91780, accuracy: 0.64000\n",
      "INFO:tensorflow:Step:  4000, loss: 0.78415, accuracy: 0.76000\n",
      "INFO:tensorflow:Step:  4100, loss: 0.53447, accuracy: 0.83000\n",
      "INFO:tensorflow:Step:  4200, loss: 0.71755, accuracy: 0.77000\n",
      "INFO:tensorflow:Step:  4300, loss: 0.81067, accuracy: 0.69000\n",
      "INFO:tensorflow:Step:  4400, loss: 0.63044, accuracy: 0.78000\n",
      "INFO:tensorflow:Step:  4500, loss: 0.61221, accuracy: 0.84000\n",
      "INFO:tensorflow:Step:  4600, loss: 0.84056, accuracy: 0.75000\n",
      "INFO:tensorflow:Step:  4700, loss: 0.51012, accuracy: 0.83000\n",
      "INFO:tensorflow:Step:  4800, loss: 0.65440, accuracy: 0.79000\n",
      "INFO:tensorflow:Step:  4900, loss: 0.65218, accuracy: 0.80000\n",
      "INFO:tensorflow:Step:  5000, loss: 0.50166, accuracy: 0.86000\n",
      "INFO:tensorflow:Step:  5100, loss: 0.56835, accuracy: 0.87000\n",
      "INFO:tensorflow:Step:  5200, loss: 0.40776, accuracy: 0.88000\n",
      "INFO:tensorflow:Step:  5300, loss: 0.35109, accuracy: 0.90000\n",
      "INFO:tensorflow:Step:  5400, loss: 0.38949, accuracy: 0.91000\n",
      "INFO:tensorflow:Step:  5500, loss: 0.44305, accuracy: 0.89000\n",
      "INFO:tensorflow:Step:  5600, loss: 0.71068, accuracy: 0.81000\n",
      "INFO:tensorflow:Step:  5700, loss: 0.54071, accuracy: 0.89000\n",
      "INFO:tensorflow:Step:  5800, loss: 0.59811, accuracy: 0.82000\n",
      "INFO:tensorflow:Step:  5900, loss: 0.59664, accuracy: 0.81000\n",
      "INFO:tensorflow:Step:  6000, loss: 0.32410, accuracy: 0.91000\n",
      "INFO:tensorflow:Step:  6100, loss: 0.48683, accuracy: 0.86000\n",
      "INFO:tensorflow:Step:  6200, loss: 0.46332, accuracy: 0.91000\n",
      "INFO:tensorflow:Step:  6300, loss: 0.42931, accuracy: 0.90000\n",
      "INFO:tensorflow:Step:  6400, loss: 0.38593, accuracy: 0.89000\n",
      "INFO:tensorflow:Step:  6500, loss: 0.27386, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  6600, loss: 0.43067, accuracy: 0.92000\n",
      "INFO:tensorflow:Step:  6700, loss: 0.54116, accuracy: 0.90000\n",
      "INFO:tensorflow:Step:  6800, loss: 0.44600, accuracy: 0.91000\n",
      "INFO:tensorflow:Step:  6900, loss: 0.31080, accuracy: 0.92000\n",
      "INFO:tensorflow:Step:  7000, loss: 0.34347, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  7100, loss: 0.18091, accuracy: 0.95000\n",
      "INFO:tensorflow:Step:  7200, loss: 0.45967, accuracy: 0.91000\n",
      "INFO:tensorflow:Step:  7300, loss: 0.22184, accuracy: 0.95000\n",
      "INFO:tensorflow:Step:  7400, loss: 0.20557, accuracy: 0.96000\n",
      "INFO:tensorflow:Step:  7500, loss: 0.28083, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  7600, loss: 0.21454, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  7700, loss: 0.24516, accuracy: 0.95000\n",
      "INFO:tensorflow:Step:  7800, loss: 0.25057, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  7900, loss: 0.18592, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  8000, loss: 0.22507, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  8100, loss: 0.24605, accuracy: 0.95000\n",
      "INFO:tensorflow:Step:  8200, loss: 0.31710, accuracy: 0.92000\n",
      "INFO:tensorflow:Step:  8300, loss: 0.05536, accuracy: 0.99000\n",
      "INFO:tensorflow:Step:  8400, loss: 0.25470, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  8500, loss: 0.09549, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  8600, loss: 0.31666, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  8700, loss: 0.09075, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  8800, loss: 0.29660, accuracy: 0.93000\n",
      "INFO:tensorflow:Step:  8900, loss: 0.26057, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  9000, loss: 0.08874, accuracy: 0.98000\n",
      "INFO:tensorflow:Step:  9100, loss: 0.19328, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  9200, loss: 0.34501, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  9300, loss: 0.21639, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  9400, loss: 0.21898, accuracy: 0.94000\n",
      "INFO:tensorflow:Step:  9500, loss: 0.11375, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  9600, loss: 0.15926, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  9700, loss: 0.19093, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  9800, loss: 0.13893, accuracy: 0.97000\n",
      "INFO:tensorflow:Step:  9900, loss: 0.10477, accuracy: 0.97000\n",
      "INFO:tensorflow:Step: 10000, loss: 0.08434, accuracy: 0.98000\n"
     ]
    }
   ],
   "source": [
    "num_train_iters = 10000\n",
    "num_test_iters = 100\n",
    "num_val_iters = 50\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init_op)\n",
    "    writer = tf.summary.FileWriter(output_folder, sess.graph)\n",
    "    for i in range(num_train_iters):\n",
    "        batch_inputs, batch_labels = train_dataset.next(hps.batch_size)\n",
    "        input_vals = (batch_inputs, batch_labels, train_keep_rate_for_dropout)\n",
    "        feed_dict = dict(zip(placeholders, input_vals))\n",
    "        fetches = [global_step, loss, accuracy, train_op, summary_op]\n",
    "        outputs = sess.run(fetches, feed_dict)\n",
    "\n",
    "        global_step_val, loss_val, accuracy_val = outputs[0:3]\n",
    "        if global_step_val % 100 == 0:\n",
    "            tf.logging.info('Step: %5d, loss: %3.5f, accuracy: %4.5f'\n",
    "                            % (global_step_val, loss_val, accuracy_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "word-level: filter=100 vocab_size=17067\n",
    "char-level: filter=100 vocab_size=3583\n",
    "        word    char\n",
    "Train   99.7%   98.9%\n",
    "Valid   92.7%   94.4%\n",
    "Test    93.2%   95%\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
