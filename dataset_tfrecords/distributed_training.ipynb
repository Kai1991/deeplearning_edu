{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "def get_tfrecord_filenames():\n",
    "    train_tfrecord_filenames = []\n",
    "    train_tfrecord_folder = 'tfrecords/train_tfrecords'\n",
    "\n",
    "    for filename in os.listdir(train_tfrecord_folder):\n",
    "        train_tfrecord_filenames.append(os.path.join(train_tfrecord_folder, filename))\n",
    "    return train_tfrecord_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset():\n",
    "    filename_placeholder = tf.placeholder(tf.string, shape=[None])\n",
    "\n",
    "    def _parse_function(example_proto):\n",
    "        features = {\"img_raw\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "                    \"label\": tf.FixedLenFeature((), tf.int64, default_value=0)}\n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        img_decoded = tf.decode_raw(parsed_features['img_raw'], tf.uint8)\n",
    "        return img_decoded, parsed_features['label']\n",
    "\n",
    "    dataset = tf.data.TFRecordDataset(filename_placeholder)\n",
    "    dataset = dataset.map(_parse_function)\n",
    "\n",
    "    batch_size = 20\n",
    "    dataset = dataset.prefetch(buffer_size=2000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.shuffle(buffer_size=2000)\n",
    "    dataset = dataset.repeat()\n",
    "    # iterator = dataset.make_one_shot_iterator()\n",
    "    iterator = dataset.make_initializable_iterator()\n",
    "    batch_imgs, batch_labels = iterator.get_next()\n",
    "    batch_imgs = tf.cast(batch_imgs, tf.float32)\n",
    "    batch_labels = tf.cast(batch_labels, tf.int64)\n",
    "    return batch_imgs, batch_labels, filename_placeholder, iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(x, y):\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        is_training = tf.placeholder(tf.bool, name=\"is_training\")\n",
    "        x_image = tf.reshape(x, [-1, 32, 32, 3])\n",
    "    with tf.name_scope(\"model\"):\n",
    "        conv1 = tf.layers.conv2d(inputs=x_image,\n",
    "                                 filters=32,\n",
    "                                 kernel_size=[5,5],\n",
    "                                 padding='same',\n",
    "                                 activation=tf.nn.relu)\n",
    "        pool1 = tf.layers.max_pooling2d(inputs=conv1,\n",
    "                                        pool_size=[2,2],\n",
    "                                        strides=2)\n",
    "\n",
    "        conv2 = tf.layers.conv2d(inputs=pool1,\n",
    "                                 filters=64,\n",
    "                                 kernel_size=[5,5],\n",
    "                                 padding='same',\n",
    "                                 activation=tf.nn.relu)\n",
    "        pool2 = tf.layers.max_pooling2d(inputs=conv2,\n",
    "                                        pool_size=[2,2],\n",
    "                                        strides=2)\n",
    "\n",
    "        conv3 = tf.layers.conv2d(inputs=pool2,\n",
    "                                 filters=64,\n",
    "                                 kernel_size=[5,5],\n",
    "                                 padding='same',\n",
    "                                 activation=tf.nn.relu)\n",
    "        pool3 = tf.layers.max_pooling2d(inputs=conv3,\n",
    "                                        pool_size=[2,2],\n",
    "                                        strides=2)\n",
    "\n",
    "        pool3_flatten = tf.layers.flatten(pool3)\n",
    "        dense = tf.layers.dense(inputs=pool3_flatten,\n",
    "                                units=128,\n",
    "                                activation=tf.nn.relu)\n",
    "        dropout = tf.layers.dropout(inputs=dense,\n",
    "                                    rate=0.1,\n",
    "                                    training=is_training)\n",
    "        logits = tf.layers.dense(inputs=dropout,\n",
    "                                units=10)\n",
    "    return is_training, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(_):\n",
    "  ps_hosts = FLAGS.ps_hosts.split(\",\")\n",
    "  worker_hosts = FLAGS.worker_hosts.split(\",\")\n",
    "\n",
    "  # Create a cluster from the parameter server and worker hosts.\n",
    "  cluster = tf.train.ClusterSpec({\"ps\": ps_hosts, \"worker\": worker_hosts})\n",
    "\n",
    "  # Create and start a server for the local task.\n",
    "  server = tf.train.Server(cluster,\n",
    "                           job_name=FLAGS.job_name,\n",
    "                           task_index=FLAGS.task_index)\n",
    "\n",
    "  if FLAGS.job_name == \"ps\":\n",
    "    server.join()\n",
    "  elif FLAGS.job_name == \"worker\":\n",
    "\n",
    "    # Assigns ops to the local worker by default.\n",
    "    with tf.device(tf.train.replica_device_setter(\n",
    "        worker_device=\"/job:worker/task:%d\" % FLAGS.task_index,\n",
    "        cluster=cluster)):\n",
    "\n",
    "      # Build model...\n",
    "      batch_imgs, batch_labels, filename_placeholder, iterator = create_dataset()\n",
    "      is_training, logits = cnn_model(batch_imgs, batch_labels)\n",
    "      loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=batch_labels, logits=logits))\n",
    "      correct_prediction = tf.equal(tf.argmax(logits, 1), batch_labels)\n",
    "      accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "      global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "\n",
    "      train_op = tf.train.AdagradOptimizer(0.001).minimize(\n",
    "          loss, global_step=global_step)\n",
    "\n",
    "    # The StopAtStepHook handles stopping after running given steps.\n",
    "    hooks=[tf.train.StopAtStepHook(last_step=1000000)]\n",
    "\n",
    "    # The MonitoredTrainingSession takes care of session initialization,\n",
    "    # restoring from a checkpoint, saving to a checkpoint, and closing when done\n",
    "    # or an error occurs.\n",
    "    with tf.train.MonitoredTrainingSession(master=server.target,\n",
    "                                           is_chief=(FLAGS.task_index == 0),\n",
    "                                           checkpoint_dir=\"./distributed_run/train_logs\",\n",
    "                                           hooks=hooks) as mon_sess:\n",
    "      mon_sess.run(iterator.initializer, feed_dict={filename_placeholder: get_tfrecord_filenames()})\n",
    "      while not mon_sess.should_stop():\n",
    "        # Run a training step asynchronously.\n",
    "        # See <a href=\"../api_docs/python/tf/train/SyncReplicasOptimizer\"><code>tf.train.SyncReplicasOptimizer</code></a> for additional details on how to\n",
    "        # perform *synchronous* training.\n",
    "        # mon_sess.run handles AbortedError in case of preempted PS.\n",
    "        loss_val, accuracy_val, _, global_step_val = mon_sess.run([loss, accuracy, train_op, global_step], feed_dict={is_training: True})\n",
    "        tf.logging.info(\"gloal_step: %d, loss_val: %3.5f, accuracy: %3.5f\" % (global_step_val, loss_val, accuracy_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "‘’‘\n",
    "在四个客户端上分别执行如下命令:\n",
    "\n",
    "python distributed_tensorflow.py \\\n",
    "    --ps_hosts=localhost:60003,localhost:60002 \\\n",
    "    --worker_hosts=localhost:60001,localhost:60000 \\\n",
    "    --job_name=worker \\\n",
    "    --task_index=0\n",
    "\n",
    "python distributed_tensorflow.py \\\n",
    "    --ps_hosts=localhost:60003,localhost:60002 \\\n",
    "    --worker_hosts=localhost:60001,localhost:60000 \\\n",
    "    --job_name=worker \\\n",
    "    --task_index=1\n",
    "\n",
    "python distributed_tensorflow.py \\\n",
    "    --ps_hosts=localhost:60003,localhost:60002 \\\n",
    "    --worker_hosts=localhost:60001,localhost:60000 \\\n",
    "    --job_name=ps \\\n",
    "    --task_index=0\n",
    "\n",
    "python distributed_tensorflow.py \\\n",
    "    --ps_hosts=localhost:60003,localhost:60002 \\\n",
    "    --worker_hosts=localhost:60001,localhost:60000 \\\n",
    "    --job_name=ps \\\n",
    "    --task_index=1\n",
    "’‘’  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.register(\"type\", \"bool\", lambda v: v.lower() == \"true\")\n",
    "  # Flags for defining the tf.train.ClusterSpec\n",
    "  parser.add_argument(\n",
    "      \"--ps_hosts\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Comma-separated list of hostname:port pairs\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--worker_hosts\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"Comma-separated list of hostname:port pairs\"\n",
    "  )\n",
    "  parser.add_argument(\n",
    "      \"--job_name\",\n",
    "      type=str,\n",
    "      default=\"\",\n",
    "      help=\"One of 'ps', 'worker'\"\n",
    "  )\n",
    "  # Flags for defining the tf.train.Server\n",
    "  parser.add_argument(\n",
    "      \"--task_index\",\n",
    "      type=int,\n",
    "      default=0,\n",
    "      help=\"Index of task within the job\"\n",
    "  )\n",
    "  FLAGS, unparsed = parser.parse_known_args()\n",
    "  tf.app.run(main=main, argv=[sys.argv[0]] + unparsed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
